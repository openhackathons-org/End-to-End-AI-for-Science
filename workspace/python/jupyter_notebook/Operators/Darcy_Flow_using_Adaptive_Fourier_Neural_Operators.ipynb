{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d872daa",
   "metadata": {},
   "source": [
    "# Solving the Darcy-Flow problem using AFNO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d331d7f8",
   "metadata": {},
   "source": [
    "In this notebook, we will introduce the brief theory behind the Adaptive Fourier Neural Operators and use them to solve the same data-driven Darcy flow problem that was introduced in the [FNO notebook](Darcy_Flow_using_Fourier_Neural_Operators.ipynb)\n",
    "\n",
    "In contrast with the Fourier Neural Operator, which has a convolutional architecture, the AFNO leverages contemporary transformer architectures in the computer vision domain. Vision transformers have delivered tremendous success in computer vision. This is primarily due to effective self-attention mechanisms. To cope with this challenge, Guibas et al. proposed <a href=\"https://www.researchgate.net/publication/356601975_Adaptive_Fourier_Neural_Operators_Efficient_Token_Mixers_for_Transformers\" rel=\"nofollow\">Adaptive Fourier Neural Operator (AFNO)</a> as an efficient attention mechanism in the Fourier Domain. AFNO is based on the principled foundation of operator learning, which allows us to frame attention as a continuous global convolution efficiently in the Fourier domain. To handle challenges in vision, such as discontinuities in images and high-resolution inputs, AFNO proposes principled architectural modifications to FNO, resulting in memory and computational efficiency. This includes imposing a block-diagonal structure on the channel mixing weights, adaptively sharing weights across tokens, and sparsifying the frequency modes via soft-thresholding and shrinkage. \n",
    "This notebook uses the AFNO transformer for modelling a PDE system. While AFNO has been designed for scaling to extremely high-resolution inputs that the FNO cannot handle as well (<a href=\"https://arxiv.org/pdf/2202.11214.pdf\" rel=\"nofollow\">FourCastNet</a>), here we present a simple example using Darcy flow. This problem is intended as an illustrative starting point for data-driven training using AFNO in Modulus, but should not be regarded as leveraging the full extent of AFNO's functionality. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e56252",
   "metadata": {},
   "source": [
    "#### Contents of the Notebook\n",
    "\n",
    "- [Theory of the Adaptive Fourier Neural Operator](#Theory-of-the-Adaptive-Fourier-Neural-Operator)\n",
    "- [Solving the Darcy-Flow problem](#Solving-the-Darcy-Flow-problem)\n",
    "    - [Problem Description](#Problem-Description)\n",
    "    - [Step 1: Loading the Data](#Step-1:-Loading-the-Data)\n",
    "    - [Step 2: Creating the nodes](#Step-2:-Creating-the-nodes)\n",
    "    - [Step 3: Creating the Domain and defining the Constraints](#Step-3:-Creating-the-Domain-and-defining-the-Constraints)\n",
    "    - [Step 4: Adding the Validator](#Step-4:-Adding-the-Validator)\n",
    "    - [Step 5: Hydra Configuration](#Step-5:-Hydra-Configuration)\n",
    "    - [Step 6: Solver and Training the model](#Step-6:-Solver-and-Training-the-model)\n",
    "    - [Visualising the solution](#Visualising-the-solution)\n",
    "\n",
    "#### Learning Outcomes\n",
    "- How to use the AFNO transformer architecture in Modulus\n",
    "- Differences between the AFNO transformer and the FNO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e69d3e",
   "metadata": {},
   "source": [
    "## Theory of the Adaptive Fourier Neural Operator\n",
    "\n",
    "The Adaptive-Fourier Neural Operator (AFNO) architecture is highly effective and computationally efficient for high-resolution inputs. It combines a key recent advance in modelling PDE systems, namely the Fourier Neural Operator (FNO), with the powerful Vision Transformer (ViT) model for image processing. FNO has shown great results in modelling PDE systems such as Navier-Stokes flows. The ViT and related variants of transformer models have achieved SOTA performance in image processing tasks. The multi-head self-attention (MHSA) mechanism of the ViT is key to its impressive performance. The self-attention mechanism models long-range interactions at each layer of the neural network, a feature that is absent in most convolutional neural networks. The drawback of the ViT self-attention architecture is that it scales as a quadratic function of the length of the token sequence and thus scales quadratically with input image resolution. The AFNO provides a solution to the scaling complexity of the ViT. The AFNO model implements a token mixing operation in the Fourier Domain. The computational complexity of the mixing operation is $\\mathcal{O}(N_{token}\\log N_{token})$ as opposed to the $\\mathcal{O}({N_{token}^2})$ complexity of the vanilla ViT architecture.\n",
    "The first step in the architecture involves dividing the input image into a regular grid with $h \\times w$ equal-sized patches of size $p\\times p$. The parameter $p$ is referred to as the patch size. For simplicity, we consider a single-channel image. Each patch is embedded into a token of size $d$, the embedding dimension. The patch embedding operation results in a token tensor ($X_{h\\times w \\times d}$) of size $h \\times w \\times d$. The patch size and embedding dimension are user-selected parameters. A smaller patch size allows the model to capture fine-scale details better while increasing the computational cost of training the model. A higher embedding dimension also increases the parameter count of the model. The token tensor is then processed by multiple layers of the transformer architecture performing spatial and channel mixing. \n",
    "The AFNO architecture implements the following operations in each layer.\n",
    "The token tensor is first transformed to the Fourier domain with\n",
    "\\begin{equation}\n",
    "z_{m,n} = [\\mathrm{DFT}(X)]_{m,n},\n",
    "\\end{equation}\n",
    "where $m,n$ is the index the patch location and DFT denotes a 2D discrete Fourier transform. The model then applies token weighting in the Fourier domain and promotes sparsity with a Soft-Thresholding and Shrinkage operation as\n",
    "\\begin{equation} \n",
    "\\tilde{z}_{m,n} = S_{\\lambda} ( \\mathrm{MLP}(z_{m,n})),\n",
    "\\end{equation}\n",
    "where $S_{\\lambda}(x) = \\mathrm{sign}(x) \\max(|x| - \\lambda, 0)$ with the sparsity controlling parameter $\\lambda$, and $\\mathrm{MLP(\\cdot)}$ is a 2-layer multi-layer perceptron with block-diagonal weight matrices which are shared across all patches. The number of blocks in the block diagonal MLP weight matrices is a user-selected hyperparameter that should be tuned appropriately. The last operation in an ANFO layer is an inverse Fourier to transform back to the patch domain and add a residual connection as\n",
    "\\begin{equation}\n",
    "y_{m,n} = [\\mathrm{IDFT}(\\tilde{Z})]_{m,n} + X_{m,n}.\n",
    "\\end{equation}\n",
    "At the end of all the transformer layers, a linear decoder converts the feature tensor back to the image space.\n",
    "There are several important hyper-parameters that affect the accuracy and computational cost of the AFNO. Empirically, the most important hyperparameters that should be tuned, keeping in mind the task at hand are the number of layers, patch size, embedding dimension and the number of blocks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee45ddcd",
   "metadata": {},
   "source": [
    "## Solving the Darcy-Flow problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b0b338",
   "metadata": {},
   "source": [
    "### Problem Description\n",
    "\n",
    "The problem description follows from the FNO notebook. We would be building a surrogate model that learns the mapping between a permeability and pressure field of a Darcy flow system. The AFNO is based on an image transformer backbone, and as with all transformer architectures, the AFNO tokenizes the input field. Each token is embedded in a patch of the image. The tokenized image is processed by the transformer layers, followed by a linear decoder which generates the output image. \n",
    "<center><img src=\"images/afno_darcy.png\" alt=\"Drawing\" style=\"width:900px\" /></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ed6d51",
   "metadata": {},
   "source": [
    "Similar to the FNO chapter, the training and validation data for this example can be found on the [Fourier Neural Operator Github page](https://github.com/zongyi-li/fourier_neural_operator). The data can be downloaded using an automated script similar to the FNO notebook. \n",
    "\n",
    "**Note:** In this notebook we will walk through the contents of [`darcy_AFNO_lazy.py`](../../source_code/darcy/darcy_AFNO.py) script. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3e29f3",
   "metadata": {},
   "source": [
    "### Step 1: Loading the Data\n",
    "\n",
    "Loading both the training and validation datasets into memory follows a similar process as seen in the FNO notebook. We will use the eager data loading for both datasets in this case. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d7b01f",
   "metadata": {},
   "source": [
    "```python\n",
    "import modulus\n",
    "from modulus.sym.hydra import instantiate_arch\n",
    "from modulus.sym.hydra.config import ModulusConfig\n",
    "from modulus.sym.key import Key\n",
    "\n",
    "from modulus.sym.domain import Domain\n",
    "from modulus.sym.domain.constraint import SupervisedGridConstraint\n",
    "from modulus.sym.domain.validator import GridValidator\n",
    "from modulus.sym.dataset import DictGridDataset\n",
    "from modulus.sym.solver import Solver\n",
    "\n",
    "from modulus.sym.utils.io.plotter import GridValidatorPlotter\n",
    "\n",
    "from utilities import download_FNO_dataset, load_FNO_dataset\n",
    "\n",
    "\n",
    "@modulus.sym.main(config_path=\"conf\", config_name=\"config_AFNO\")\n",
    "def run(cfg: ModulusConfig) -> None:\n",
    "\n",
    "    # load training/ test data\n",
    "    input_keys = [Key(\"coeff\", scale=(7.48360e00, 4.49996e00))]\n",
    "    output_keys = [Key(\"sol\", scale=(5.74634e-03, 3.88433e-03))]\n",
    "\n",
    "    download_FNO_dataset(\"Darcy_241\", outdir=\"datasets/\")\n",
    "    invar_train, outvar_train = load_FNO_dataset(\n",
    "        \"datasets/Darcy_241/piececonst_r241_N1024_smooth1.hdf5\",\n",
    "        [k.name for k in input_keys],\n",
    "        [k.name for k in output_keys],\n",
    "        n_examples=1000,\n",
    "    )\n",
    "    invar_test, outvar_test = load_FNO_dataset(\n",
    "        \"datasets/Darcy_241/piececonst_r241_N1024_smooth2.hdf5\",\n",
    "        [k.name for k in input_keys],\n",
    "        [k.name for k in output_keys],\n",
    "        n_examples=100,\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22558b04",
   "metadata": {},
   "source": [
    "The inputs for AFNO need to be perfectly divisible by the specified patch size (in this case, `patch_size=16`), which is not the case for this dataset. Therefore, we trim the input/output features such that they are have appropriate dimensionality `241x241 -> 240x240`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e4addf",
   "metadata": {},
   "source": [
    "```python\n",
    "    # get training image shape\n",
    "    img_shape = [\n",
    "        next(iter(invar_train.values())).shape[-2],\n",
    "        next(iter(invar_train.values())).shape[-1],\n",
    "    ]\n",
    "\n",
    "    # crop out some pixels so that img_shape is divisible by patch_size of AFNO\n",
    "    img_shape = [s - s % cfg.arch.afno.patch_size for s in img_shape]\n",
    "    print(f\"cropped img_shape: {img_shape}\")\n",
    "    for d in (invar_train, outvar_train, invar_test, outvar_test):\n",
    "        for k in d:\n",
    "            d[k] = d[k][:, :, : img_shape[0], : img_shape[1]]\n",
    "            print(f\"{k}: {d[k].shape}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e8c83a",
   "metadata": {},
   "source": [
    "### Step 2: Creating the nodes\n",
    "\n",
    "Initializing the model and domain again follow the same steps as seen in the FNO notebook. For AFNO, we calculate the size of the domain after loading the dataset. The domain needs to be defined in the AFNO model, which is provided with the inclusion of the keyword argument `img_shape` in the `instantiate_arch` call. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594268b7",
   "metadata": {},
   "source": [
    "```python\n",
    "    # make list of nodes to unroll graph on\n",
    "    model = instantiate_arch(\n",
    "        input_keys=input_keys,\n",
    "        output_keys=output_keys,\n",
    "        cfg=cfg.arch.afno,\n",
    "        img_shape=img_shape,\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24c2ac8",
   "metadata": {},
   "source": [
    "### Step 3: Creating the Domain and defining the Constraints \n",
    "\n",
    "The data-driven constraints and validators are then added to the domain in the same fashion as the FNO notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f737639",
   "metadata": {},
   "source": [
    "```python\n",
    "    # add constraints to domain\n",
    "    supervised = SupervisedGridConstraint(\n",
    "        nodes=nodes,\n",
    "        dataset=train_dataset,\n",
    "        batch_size=cfg.batch_size.grid,\n",
    "    )\n",
    "    domain.add_constraint(supervised, \"supervised\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0a0206",
   "metadata": {},
   "source": [
    "### Step 4: Adding the Validator\n",
    "\n",
    "We can now proceed and add the Validators in the same fashion as in the previous notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691c59b9",
   "metadata": {},
   "source": [
    "```python\n",
    "    # add validator\n",
    "    val = GridValidator(\n",
    "        nodes,\n",
    "        dataset=test_dataset,\n",
    "        batch_size=cfg.batch_size.validation,\n",
    "        plotter=GridValidatorPlotter(n_examples=5),\n",
    "    )\n",
    "    domain.add_validator(val, \"test\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34c03e0",
   "metadata": {},
   "source": [
    "### Step 5: Hydra Configuration\n",
    "\n",
    "The AFNO is based on the ViT transformer architecture and requires tokenization of the inputs. Here each token is a patch of the image with a patch size defined in the configuration file through the parameter `patch_size`. The `embed_dim` parameter defines the size of the latent embedded features used inside the model for each patch. The contents of the [`config_AFNO.yaml`](../../source_code/darcy/conf/config_AFNO.yaml) are shown below. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b632b5f",
   "metadata": {},
   "source": [
    "```yaml\n",
    "defaults :\n",
    "  - modulus_default\n",
    "  - arch:\n",
    "      - afno\n",
    "  - scheduler: tf_exponential_lr\n",
    "  - optimizer: adam\n",
    "  - loss: sum\n",
    "  - _self_\n",
    "\n",
    "arch:\n",
    "  afno:\n",
    "    patch_size: 16\n",
    "    embed_dim: 256\n",
    "    depth: 4\n",
    "    num_blocks: 8\n",
    "    \n",
    "scheduler:\n",
    "  decay_rate: 0.95\n",
    "  decay_steps: 1000\n",
    "\n",
    "training:\n",
    "  rec_results_freq : 1000\n",
    "  max_steps : 10000\n",
    "\n",
    "batch_size:\n",
    "  grid: 32\n",
    "  validation: 32\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19896cb",
   "metadata": {},
   "source": [
    "### Step 6: Solver and Training the model\n",
    "\n",
    "Once the domain and the configuration is set up, the `Solver` can be defined and the training can be started as seen in earlier notebooks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d667c9a",
   "metadata": {},
   "source": [
    "```python\n",
    "    # make solver\n",
    "    slv = Solver(cfg, domain)\n",
    "\n",
    "    # start solver\n",
    "    slv.solve()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0547de",
   "metadata": {},
   "source": [
    "Before we can start training, we can make use of Tensorboard for visualizing the loss values and convergence of several other monitors we just created. This can be done inside the Jupyter framework by selecting the directory in which the checkpoint will be stored by clicking on the small checkbox next to it. The option to launch a Tensorboard then shows up in that directory. Once you open Tensorboard, switch between the SCALARS , IMAGES , TEXT , TIME SERIES to visualise and view Validation and other information related to Training.\n",
    "\n",
    "For this application, please verify if you are inside the `/jupyter_notebook/Operators` folder after launching Tensorboard.\n",
    "\n",
    "\n",
    "1. The option to launch a Tensorboard then shows up in that directory.\n",
    "\n",
    "<center><img src=\"../projectile/images/tensorboard.png\" alt=\"Drawing\" style=\"width:900px\" /></center>\n",
    "\n",
    "2. We can launch tensorboard using the following command: \n",
    "\n",
    "```\n",
    "tensorboard --logdir /workspace/python/jupyter_notebook/ --port 8889\n",
    "```\n",
    "\n",
    "3. Open a new tab in your browser and head to [http://127.0.0.1:8889](http://127.0.0.1:8889) . You should see a screen similar to the below one. \n",
    "\n",
    "<center><img src=\"../projectile/images/tensorboard_browser.png\" alt=\"Drawing\" style=\"width:900px\" /></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0d4d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"RANK\"]=\"0\"\n",
    "os.environ[\"WORLD_SIZE\"]=\"1\"\n",
    "os.environ[\"MASTER_ADDR\"]=\"localhost\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9fe075",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../../source_code/darcy/darcy_AFNO.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ba21a0",
   "metadata": {},
   "source": [
    "### Visualising the solution\n",
    "\n",
    "The checkpoint directory is saved based on the results recording frequency specified in the `rec_results_freq` parameter of its derivatives. The network directory folder contains several plots of the different validation predictions, some of which are shown below. \n",
    "\n",
    "AFNO validation predictions. (Left to right) Input permeability, true pressure, predicted pressure, error.\n",
    "\n",
    "<center><img src=\"images/afno_darcy_pred1.png\" alt=\"Drawing\" style=\"width: 900px;\"/></center>\n",
    "<center><img src=\"images/afno_darcy_pred2.png\" alt=\"Drawing\" style=\"width: 900px;\"/></center>\n",
    "<center><img src=\"images/afno_darcy_pred3.png\" alt=\"Drawing\" style=\"width: 900px;\"/></center>\n",
    "\n",
    "It is important to recognize that AFNO's strengths lie in its ability to scale to much larger model sizes and datasets than what is used in this notebook/example. While not illustrated here, this example demonstrates the fundamental implementation of data-driven training using the AFNO architecture in Modulus for you to extend to larger problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987b118f",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "Don't forget to check out additional [Open Hackathons Resources](https://www.openhackathons.org/s/technical-resources) and join our [OpenACC and Hackathons Slack Channel](https://www.openacc.org/community#slack) to share your experience and get more help from the community.\n",
    "\n",
    "---\n",
    "\n",
    "# Licensing\n",
    "\n",
    "Copyright Â© 2023 OpenACC-Standard.org.  This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0). These materials may include references to hardware and software developed by other entities; all applicable licensing and copyrights apply."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
