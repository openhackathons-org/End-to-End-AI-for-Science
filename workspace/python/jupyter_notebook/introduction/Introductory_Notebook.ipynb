{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Physics Informed Neural Networks (PINNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will outline the brief theory about Physics Informed Neural Networks (PINNs).\n",
    "\n",
    "#### Contents of the Notebook\n",
    "- [Neural Network Solver Methodology](#Neural-Network-Solver-Methodology)\n",
    "- [Parameterized Problems](#Parameterized-Problems)\n",
    "- [Inverse Problems](#Inverse-Problems)\n",
    "    \n",
    "\n",
    "#### Learning Outcomes\n",
    "- Theory of Physics Informed Neural Networks\n",
    "- How loss function for PINNs are constructed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Solver Methodology \n",
    "\n",
    "In this section, we provide a brief introduction to solving differential equations with neural networks. The idea is to use a neural network to approximate the solution to the given differential equation and boundary conditions. We train this neural network by constructing a loss function for how well the neural network is satisfying the differential equation and boundary conditions. If the network can minimize this loss function, then it will in effect, solve the given differential equation.\n",
    "To illustrate this idea, we will give an example of solving the following problem,\n",
    "\\begin{equation} \\label{1d_equation}\n",
    "    \\mathbf{P} : \\left\\{\\begin{matrix}\n",
    "\\frac{\\mathrm{d}^2 u}{\\mathrm{d} x^2}(x) = f(x), \\\\ \n",
    "\\\\\n",
    "u(0) = u(1) = 0,\n",
    "\\end{matrix}\\right.\n",
    "\\end{equation}\n",
    "We start by constructing a neural network $u_{net}(x)$. The input to this network is a single value $x \\in \\mathbb{R}$, and its output is also a single value $u_{net}(x) \\in \\mathbb{R}$. We suppose that this neural network is infinitely differentiable, $u_{net} \\in C^{\\infty}$. The typical neural network used is a deep fully connected network where the activation functions are infinitely differentiable. \n",
    "Next, we need to construct a loss function to train this neural network. We easily encode the boundary conditions as a loss in the following way:\n",
    "\\begin{equation}\n",
    "  L_{BC} = u_{net}(0)^2 + u_{net}(1)^2\n",
    "\\end{equation}\n",
    "For encoding the equations, we need to compute the derivatives of $u_{net}$. Using automatic differentiation we can do so and compute $\\frac{\\mathrm{d}^2 u_{net}}{\\mathrm{d} x^2}(x)$. This allows us to write a loss function of the form:\n",
    "\\begin{equation} \\label{sumation_loss}\n",
    "  L_{residual} = \\frac{1}{N}\\sum^{N}_{i=0} \\left( \\frac{\\mathrm{d}^2 u_{net}}{\\mathrm{d} x^2}(x_i) - f(x_i) \\right)^2\n",
    "\\end{equation}\n",
    "Where the $x_i$'s are a batch of points sampled in the interior, $x_i \\in (0, 1)$. Our total loss is $L = L_{BC} + L_{residual}$. Optimizers such as Adam are used to train this neural network. Given $f(x)=1$, the true solution is $\\frac{1}{2}(x-1)x$. Upon solving the problem, you can obtain good agreement between the exact solution and the neural network solution,the as shown in Figure below.\n",
    "<center><img src=\"images/single_parabola.png\" alt=\"Drawing\" style=\"width:500px\" /></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameterized Problems\n",
    "\n",
    "One important advantage of a neural network solver over traditional numerical methods is its ability to solve parameterized geometries. To illustrate this concept, we solve a parameterized version of the above problem. Suppose we want to know how the solution to this equation changes as we move the position on the boundary condition $u(l)=0$. We can parameterize this position with a variable $l \\in [1,2]$ and our equation now has the form,\n",
    "\\begin{equation} \\label{1d_equation2}\n",
    "    \\mathbf{P} : \\left\\{\\begin{matrix}\n",
    "\\frac{\\mathrm{d}^2 u}{\\mathrm{d} x^2}(x) = f(x), \\\\ \n",
    "\\\\\n",
    "u(0) = u(l) = 0,\n",
    "\\end{matrix}\\right.\n",
    "\\end{equation}\n",
    "To solve this parameterized problem, we can have the neural network take $l$ as input, $u_{net}(x,l)$. The losses then take the form \n",
    "\\begin{equation}\n",
    "  L_{residual} = \\int_1^2 \\int_0^l \\left( \\frac{\\mathrm{d}^2 u_{net}}{\\mathrm{d} x^2}(x,l) - f(x) \\right)^2 dx dl \\approx \\left(\\int_1^2 \\int^l_0 dxdl\\right) \\frac{1}{N} \\sum^{N}_{i=0} \\left(\\frac{\\mathrm{d}^2 u_{net}}{\\mathrm{d} x^2}(x_i, l_i) - f(x_i)\\right)^2\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "  L_{BC} = \\int_1^2 (u_{net}(0,l))^2 + (u_{net}(l,l))^2 dl \\approx \\left(\\int_1^2 dl\\right) \\frac{1}{N} \\sum^{N}_{i=0} (u_{net}(0, l_i))^2 + (u_{net}(l_i, l_i))^2\n",
    "\\end{equation}\n",
    "In the figure below, we see the solution to the differential equation for various $l$ values after optimizing the network on this loss. While this example problem is overly simplistic, the ability to solve parameterized geometries presents significant industrial value. Instead of performing a single simulation, we can solve multiple designs simultaneously for reduced computational cost. More examples of this can be found in <a href=\"https://docs.nvidia.com/deeplearning/modulus/index.html\" rel=\"nofollow\">Modulus User Documentation</a>.\n",
    "<center><img src=\"images/every_parabola.png\" alt=\"Drawing\" style=\"width:500px\" /></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse Problems \n",
    "\n",
    "Another useful application of a neural network solver is solving inverse problems. In an inverse problem, we start with a set of observations and then use those observations to calculate the causal factors that produced them. To illustrate how to solve inverse problems with a neural network solver, we give the example of inverting out the source term $f(x)$ from the same equation from the above problem. Suppose we are given the solution $u_{true}(x)$ at 100 random points between 0 and 1, and we want to determine the $f(x)$ that is causing it. We can do this by making two neural networks $u_{net}(x)$ and $f_{net}(x)$ to approximate both $u(x)$ and $f(x)$. These networks are then optimized to minimize the following losses;\n",
    "\\begin{equation}\n",
    "  L_{residual} \\approx \\left(\\int^1_0 dx\\right) \\frac{1}{N} \\sum^{N}_{i=0} \\left(\\frac{\\mathrm{d}^2 u_{net}}{\\mathrm{d} x^2}(x_i, l_i) - f_{net}(x_i)\\right)^2\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "  L_{data} = \\frac{1}{100} \\sum^{100}_{i=0} (u_{net}(x_i) - u_{true}(x_i))^2\n",
    "\\end{equation}\n",
    "Using the function $u_{true}(x)=\\frac{1}{48} (8 x (-1 + x^2) - (3 sin(4 \\pi x))/\\pi^2)$ the solution for $f(x)$ is $x + sin(4 \\pi x)$. We solve this problem and compare the results in the Figures below.\n",
    "Comparison of true solution for $f(x)$ and the function approximated by the NN:\n",
    "<center><img src=\"images/inverse_parabola.png\" alt=\"Drawing\" style=\"width:500px\" /><center>\n",
    "\n",
    "Comparison of $u_{net}(x)$ and the train points from $u_{true}$:\n",
    "<center><img src=\"images/inverse_parabola_2.png\" alt=\"Drawing\" style=\"width:500px\" /><center>\n",
    "\n",
    "More examples of solving an inverse problem can be found in the <a href=\"https://docs.nvidia.com/deeplearning/modulus/index.html\" rel=\"nofollow\">Modulus User Documentation</a>.\n",
    "</center></center></center></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "Don't forget to check out additional [Open Hackathons Resources](https://www.openhackathons.org/s/technical-resources) and join our [OpenACC and Hackathons Slack Channel](https://www.openacc.org/community#slack) to share your experience and get more help from the community.\n",
    "\n",
    "---\n",
    "\n",
    "# Licensing\n",
    "\n",
    "Copyright Â© 2023 OpenACC-Standard.org.  This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0). These materials may include references to hardware and software developed by other entities; all applicable licensing and copyrights apply.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
