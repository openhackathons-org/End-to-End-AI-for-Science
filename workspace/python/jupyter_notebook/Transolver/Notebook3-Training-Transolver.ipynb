{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4780454-608f-4008-99c0-f9569c0405de",
   "metadata": {},
   "source": [
    "\n",
    "# Notebook 3: From Blueprint to Factory — Training Transolver\n",
    "\n",
    "Welcome to the final notebook in our series!\n",
    "\n",
    "* In **Notebook 1 (\"The Why\")**, we established the core problem: standard Transformers have a \"quadratic complexity\" ($O(N^2)$). This makes them \"computationally prohibited\" for the \"large-scale meshes\" used in real-world physics. We saw that physics is mostly local, making this all-to-all attention incredibly wasteful.\n",
    "\n",
    "* In **Notebook 2 (\"The How\")**, we built a `NumPy` blueprint of the Transolver solution: **Physics-Attention**. We saw the 4-step mathematical process (Slice, Aggregate, Attend, Deslice) that breaks the quadratic bottleneck and achieves **linear complexity** ($O(N)$).\n",
    "\n",
    "### The Goal of This Notebook\n",
    "\n",
    "Our goal is to use **PhysicsNEMO** training pipeline to train the Transolver model on our **508 Ahmed body simulations**.\n",
    "\n",
    "This notebook is a \"guided tour\" of this pipeline. We will see how all the `.py` scripts you have (`datapipe.py`, `train.py`, etc.) in Physicsnemo for transolver work together.\n",
    "**For this educational notebook, we will run the training for only a few epochs to confirm the entire pipeline works. For a full, production-level training run, we recommend using the complete scripts from the official Transolver GitHub repository.**\n",
    "\n",
    "\n",
    "### The Workflow: A 3-Stage Process\n",
    "\n",
    "We will follow a 3-stage process to go from raw VTP/STL files to a trained model.\n",
    "\n",
    "**Stage 1: Preprocessing (VTP/STL $\\to$ Zarr)**\n",
    "* **The Problem:** The high-performance `datapipe.py` script is a `ZarrDataset` reader. It does *not* know how to read your VTP/STL files.\n",
    "* **Our Solution:** We will first run a script that loops through all 508 of our VTP/STL files. This script will:\n",
    "    1.  Read each VTP/STL pair.\n",
    "    2.  Extract all the arrays the model needs (e.g., `surface_mesh_centers`, `surface_normals`, `surface_fields`).\n",
    "    3.  Save them in the efficient `.zarr` format.\n",
    "* We will also split our 508 files into a `train` set (~457 files) and a `val` set (~51 files) so we can check for overfitting.\n",
    "\n",
    "**Stage 2: Normalization (`compute_normalizations.py`)**\n",
    "* **The Problem:** Our target data, `surface_fields`, contains both **pressure** and **wall shear stress**. These have vastly different scales (e.g., 1000 Pa vs. 0.5 $N/m^2$). If we don't fix this, the model will only learn to predict the \"loud\" pressure data and will ignore the \"quiet\" WSS.\n",
    "* **The Solution:** We will run a script that loops through our new `train` Zarr dataset and computes the global **mean** and **standard deviation** for our target fields. This saves a `surface_fields_normalization.npz` file, which `train.py` uses to \"z-score\" the targets, ensuring the model gives them equal importance.\n",
    "\n",
    "**Stage 3: Training (`train.py`)**\n",
    "* **This is the main event.** We will adapt the `train.py` script to run in this notebook. This \"factory manager\" script does the following every step:\n",
    "    1.  Uses `datapipe.py` to efficiently load a Zarr file.\n",
    "    2.  Uses `preprocess.py` to prepare the data (e.g., center the car, normalize the targets using our `.npz` file).\n",
    "    3.  Performs the **forward pass** by sending the data through the `Transolver` model.\n",
    "    4.  Uses `loss.py` to calculate the error (the \"loss\").\n",
    "    5.  Performs the **backward pass** (learning) and updates the model's weights.\n",
    "    6.  Uses `metrics.py` to calculate human-readable L2 errors on the *un-normalized* data so we can see how well it's *really* doing.\n",
    "\n",
    "### What Is the Model *Really* Learning?\n",
    "\n",
    "As we run the training, remember the core idea from the Transolver paper. We are not just fitting points. We are training the model to move beyond the \"superficial and unwieldy meshes\".\n",
    "\n",
    "The model is learning to:\n",
    "1.  **\"Ascribe\"** all the mesh points into $M$ \"learnable slices\".\n",
    "2.  **Discover** the \"intrinsic physical states\" of the flow, grouping physically similar points.\n",
    "3.  **Learn** to group spatially distant but physically-related points, like the **windshield, license plate, and headlight**, into the same \"front-facing\" slice.\n",
    "\n",
    "This is what gives Transolver its power. Let's begin.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f18615-0c8e-4fe3-bf92-b2edea452a33",
   "metadata": {},
   "source": [
    "## Library Imports\n",
    "The code block below is the main setup cell, responsible for importing all the necessary tools for the notebook. It's divided into several groups: standard Python libraries (os, glob, random) for file finding and shuffling; core data science and machine learning libraries (numpy, torch, torchinfo) for handling data and building the neural network; specialized simulation libraries (pyvista, zarr, tqdm) for reading your 3D VTP/STL files, writing them to the efficient Zarr format, and displaying progress bars; and performance utilities (torch.amp, tabulate) for speeding up training and printing clean results.\n",
    "\n",
    "The most critical section is the try...except block, which imports the actual Transolver model from the physicsnemo library and all the custom Python scripts (datapipe.py, loss.py, etc.) that define our training pipeline. This block ensures all necessary code is present before we begin. Finally, the cell sets random seeds (np.random.seed(42)) to make our training reproducible, and it configures pyvista with a \"virtual screen\" (pv.start_xvfb()) so it can process 3D meshes in a headless server environment like this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "678e9454-98a1-4b64-a74f-e84a482044e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/pyvista/plotting/utilities/xvfb.py:48: PyVistaDeprecationWarning: This function is deprecated and will be removed in future version of PyVista. Use vtk-osmesa instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Core ML/Data Libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.amp import autocast, GradScaler\n",
    "from tabulate import tabulate\n",
    "import torchinfo\n",
    "\n",
    "# Visualization & VTP/STL Reading\n",
    "import pyvista as pv\n",
    "import zarr\n",
    "from tqdm import tqdm\n",
    "from IPython.display import Markdown as md, display\n",
    "#  Physics-NEMO Imports\n",
    "try:\n",
    "    from physicsnemo.models.transolver import Transolver\n",
    "    from physicsnemo.distributed import DistributedManager\n",
    "    from physicsnemo.launch.logging import PythonLogger, RankZeroLoggingWrapper\n",
    "    \n",
    "# Import from the 'utils' directory\n",
    "    from utils.datapipe import DomainParallelZarrDataset\n",
    "    from utils.loss import loss_fn\n",
    "    from utils.metrics import metrics_fn\n",
    "    from utils.preprocess import preprocess_surface_data, downsample_surface\n",
    "    from utils.compute_normalizations import compute_mean_std_min_max\n",
    "   \n",
    "except ImportError as e:\n",
    "    print(\"=\"*50)\n",
    "    print(\"ERROR: Could not import Physics-NEMO files.\")\n",
    "    print(\"Please ensure physicsnemo is installed and all .py scripts\")\n",
    "    print(\"(datapipe.py, loss.py, etc.) are in the same directory as this notebook.\")\n",
    "    print(f\"Details: {e}\")\n",
    "    print(\"=\"*50)\n",
    "    # This will stop the notebook if imports fail\n",
    "    raise e\n",
    "\n",
    "# Set consistent random seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Setup for PyVista plotting in a headless environment\n",
    "pv.start_xvfb()\n",
    "os.environ[\"XDG_RUNTIME_DIR\"] = \"/tmp\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82189c4a-3df0-4cca-bc78-0a15ffd72029",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "This first cell is the most important one for you to edit. It acts as the central **\"control panel\"** for the entire notebook. You must set all the paths and parameters here before running any other part.\n",
    "\n",
    "Here is a breakdown of what each section does:\n",
    "\n",
    "* **1. Data Input Paths (`VTP_DIR`, `STL_DIR`):**\n",
    "    * This tells the notebook where to **find** your 508 raw simulation files.\n",
    "    * `VTP_DIR` points to the folder containing your `.vtp` files (which have the pressure and wall shear stress data).\n",
    "    * `STL_DIR` points to the folder containing your `.stl` files (which have the base geometry).\n",
    "\n",
    "* **2. Data Output Paths (`ZARR_OUTPUT_DIR`, `CHECKPOINT_DIR`, `LOG_DIR`):**\n",
    "    * This tells the notebook where to **save** all the new files it creates.\n",
    "    * `ZARR_OUTPUT_DIR` is the main folder where the script will save the processed `.zarr` files. Zarr is a high-performance format that the `datapipe.py` script knows how to read quickly during training.\n",
    "    * `CHECKPOINT_DIR` is where the trained model files (your `.pt` or `.pth` files) will be saved after each epoch.\n",
    "    * `LOG_DIR` is where data for TensorBoard (a tool for visualizing training loss) will be stored.\n",
    "\n",
    "* **3. Splitting (`TRAIN_SPLIT_RATIO`):**\n",
    "    * This is a crucial machine learning step. We can't train and test the model on the same data.\n",
    "    * This line splits your 508 files into two groups:\n",
    "        * **Training Set (90%, ~457 files):** The data the model *learns* from.\n",
    "        * **Validation Set (10%, ~51 files):** The data the model is *tested* on to see if it's *actually* learning the physics or just \"memorizing\" the training files.\n",
    "\n",
    "* **4. Simulation Parameters (`AIR_DENSITY`, `STREAM_VELOCITY`):**\n",
    "    * These are **critical inputs** that are *not* in your VTP/STL files.\n",
    "    * The model needs to know the physical conditions of the simulation. These are \"functional features\" that are fed into the model along with the geometry.\n",
    "    * You must set these to the correct constant values used in your simulations.\n",
    "\n",
    "* **5. Model & Training Hyperparameters (`C_MODEL`, `M_SLICES`, etc.):**\n",
    "    * These variables define the **architecture of the Transolver model** and the **training plan**.\n",
    "    * `C_MODEL` (`n_hidden` in the Transolver paper ) sets the feature size inside the model.\n",
    "    * `M_SLICES` (`slice_num` in the Transolver paper) sets the number of \"physics-aware slices\" ($M$)—the core idea of Transolver.\n",
    "    * `NUM_LAYERS` and `NUM_HEADS` define the depth and width of the Transformer.\n",
    "    * `NUM_EPOCHS = 5` is set low *specifically for this educational notebook* so it runs quickly. A real training run would use hundreds of epochs.\n",
    "\n",
    "* **Final Section (`os.makedirs`):**\n",
    "    * This is a \"housekeeping\" step. It automatically creates the `train/`, `val/`, and `checkpoints/` directories you defined in Section 2.\n",
    "    * This ensures the script doesn't crash later when it tries to save a file to a folder that doesn't exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcb5df26-ecab-469c-8dce-9d6a3b1129f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Configuration Loaded ---\n",
      "Training Zarr Path: /workspace/physicsnemo_ahmed_body_dataset_vv1/dataset/ahmed_body_zarr/train\n",
      "Validation Zarr Path: /workspace/physicsnemo_ahmed_body_dataset_vv1/dataset/ahmed_body_zarr/val\n",
      "Checkpoint Path: ./checkpoints/ahmed_body_run\n"
     ]
    }
   ],
   "source": [
    "# 1. Data Input Paths\n",
    "# Point this to the directory containing your 508 VTP files\n",
    "VTP_DIR = \"/workspace/physicsnemo_ahmed_body_dataset_vv1/dataset/train\"\n",
    "# Point this to the directory containing your 508 STL files\n",
    "STL_DIR = \"/workspace/physicsnemo_ahmed_body_dataset_vv1/dataset/train_stl_files\"\n",
    "\n",
    "# 2. Data Output Paths\n",
    "# This is where all the processed Zarr files will be saved\n",
    "ZARR_OUTPUT_DIR = \"/workspace/physicsnemo_ahmed_body_dataset_vv1/dataset/ahmed_body_zarr\"\n",
    "# This is where the final model checkpoints will be saved\n",
    "CHECKPOINT_DIR = \"./checkpoints/\"\n",
    "# This is where TensorBoard logs will be saved\n",
    "LOG_DIR = \"./logs/\"\n",
    "\n",
    "# 3. Splitting\n",
    "# We will split your 508 files into a training and validation set\n",
    "# 0.9 = 90% for training (~457 files), 10% for validation (~51 files)\n",
    "TRAIN_SPLIT_RATIO = 0.9\n",
    "\n",
    "# 4. Simulation Parameters\n",
    "# The model NEEDS these. You must provide the *correct* values\n",
    "AIR_DENSITY = 1.184\n",
    "STREAM_VELOCITY = 25.0\n",
    "\n",
    "# 5. Model & Training Hyperparameters\n",
    "# These are small for a fast \"educational\" run.\n",
    "# For a real run, use the values from the paper (e.g., 512, 12, 8)\n",
    "C_MODEL = 64   # Feature dimension (transolver paper uses 512)\n",
    "M_SLICES = 16  # Number of slices (transolver paper uses 64)\n",
    "NUM_LAYERS = 4 # Number of layers (transolver paper uses 12)\n",
    "NUM_HEADS = 4  # Number of attention heads (transolver paper uses 8)\n",
    "NUM_EPOCHS = 5 # Run for just 5 epochs\n",
    "PRECISION = \"float32\" # \"float16\" or \"bfloat16\" for faster training\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "\n",
    "# Create all the output directories\n",
    "# These are the directories the training script expects\n",
    "TRAIN_ZARR_DIR = os.path.join(ZARR_OUTPUT_DIR, \"train\")\n",
    "VAL_ZARR_DIR = os.path.join(ZARR_OUTPUT_DIR, \"val\")\n",
    "CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, \"ahmed_body_run\")\n",
    "LOG_PATH = os.path.join(LOG_DIR, \"ahmed_body_run\")\n",
    "\n",
    "os.makedirs(TRAIN_ZARR_DIR, exist_ok=True)\n",
    "os.makedirs(VAL_ZARR_DIR, exist_ok=True)\n",
    "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "os.makedirs(LOG_PATH, exist_ok=True)\n",
    "\n",
    "print(\"--- Configuration Loaded ---\")\n",
    "print(f\"Training Zarr Path: {TRAIN_ZARR_DIR}\")\n",
    "print(f\"Validation Zarr Path: {VAL_ZARR_DIR}\")\n",
    "print(f\"Checkpoint Path: {CHECKPOINT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e0f6a0-293e-4ea1-961c-afa590e8e2cb",
   "metadata": {},
   "source": [
    "## Preprocessing (VTP/STL -> Zarr)\n",
    "This code block is the crucial preprocessing step (Stage 1), responsible for converting your 508 raw VTP/STL files into the efficient Zarr format that the Physics-NEMO datapipe requires. The main loop at the bottom of the script acts as the \"manager\": it first finds and matches all 508 of your VTP and STL file pairs, shuffles them randomly to prevent bias, and then splits them into a training set (90% of the data) and a validation set (10%). It then uses tqdm to show a progress bar as it iterates through these two new lists, passing each file pair to the process_one_simulation function, which saves the converted files into the TRAIN_ZARR_DIR and VAL_ZARR_DIR folders you defined in Part 1.\n",
    "\n",
    "The process_one_simulation function is the \"worker\" that handles the actual conversion for each simulation. It uses pyvista to open both files: from the STL file, it extracts cell-based geometry data (the stl_areas and stl_centers for each triangle), which the model uses later to find the center of mass. From the VTP file, it extracts the critical point-based data that the model actually trains on: the 3D coordinates (surface_mesh_centers), the normal vector for each point (surface_normals), and the target physics values (pressure and wallShearStress). It then stacks the pressure and WSS into a single surface_fields array and saves all these extracted arrays—plus the scalar air_density and stream_velocity—into a new, single .zarr file using the root.create_array command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6126b21f-0d2d-4877-beeb-fdb69e5e214d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STAGE 1: PREPROCESSING ---\n",
      "Found 408 matching VTP/STL simulation pairs.\n",
      "Splitting data: 367 training, 41 validation.\n",
      "\n",
      "Processing TRAINING files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Convert: 100%|██████████| 367/367 [03:06<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing VALIDATION files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val Convert: 100%|██████████| 41/41 [00:20<00:00,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STAGE 1: COMPLETE ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def process_one_simulation(stl_path, vtp_path, zarr_path, density, velocity):\n",
    "    \"\"\"\n",
    "    Loads one VTP/STL pair, extracts data, and saves to a Zarr file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. PROCESS STL FILE\n",
    "        stl_mesh = pv.get_reader(stl_path).read()\n",
    "        \n",
    "        # Explicitly compute cell sizes for area\n",
    "        stl_areas_data = stl_mesh.compute_cell_sizes(length=False, area=True, volume=False)\n",
    "        stl_areas = np.array(stl_areas_data.cell_data[\"Area\"])\n",
    "        \n",
    "        # Call cell_centers() as a function\n",
    "        stl_centers = stl_mesh.cell_centers().points\n",
    "        \n",
    "        if stl_areas.shape[0] == 0 or stl_centers.shape[0] == 0:\n",
    "            raise ValueError(f\"STL file {stl_path} is empty.\")\n",
    "            \n",
    "        #  2. PROCESS VTP FILE (POINT-BASED WORKFLOW)\n",
    "        vtp_mesh = pv.read(vtp_path)\n",
    "        \n",
    "        # Get POINT coordinates (N_points, 3)\n",
    "        surface_mesh_centers = vtp_mesh.points\n",
    "        \n",
    "        # Compute POINT normals\n",
    "        vtp_mesh.compute_normals(cell_normals=False, point_normals=True, \n",
    "                                 auto_orient_normals=True)\n",
    "        \n",
    "        # Access the .point_normals property\n",
    "        surface_normals = vtp_mesh.point_normals\n",
    "        \n",
    "        # Check for our target POINT data keys\n",
    "        if 'p' not in vtp_mesh.point_data or 'wallShearStress' not in vtp_mesh.point_data:\n",
    "            print(f\"Warning: VTP file {vtp_path} missing POINT data 'p' or 'wallShearStress'. \"\n",
    "                  f\"Skipping. Keys: {vtp_mesh.array_names}\")\n",
    "            return False\n",
    "            \n",
    "        # Get POINT fields (N_points, 1) and (N_points, 3)\n",
    "        pressure = vtp_mesh.point_data['p']\n",
    "        wss = vtp_mesh.point_data['wallShearStress']\n",
    "        \n",
    "        # Reshape to (N, 1) and (N, 3)\n",
    "        if pressure.ndim == 1:\n",
    "            pressure = pressure.reshape(-1, 1)\n",
    "        if wss.ndim == 1 or wss.shape[1] != 3:\n",
    "            raise ValueError(f\"wallShearStress shape is {wss.shape}, but (N, 3) is required.\")\n",
    "            \n",
    "        # Stack to (N_points, 4) -> (Pressure, WSS_x, WSS_y, WSS_z)\n",
    "        surface_fields = np.hstack((pressure, wss)).astype(np.float32)\n",
    "\n",
    "        # 3. SAVE TO ZARR FILE\n",
    "        root = zarr.open_group(zarr_path, mode='w')\n",
    "        \n",
    "        # --- FIX: Removed the extra batch dimension [None, ...] ---\n",
    "        # The datapipe will add the batch dimension.\n",
    "        # Save data as (N_points, C) or (N_cells, C)\n",
    "        root.create_array('surface_mesh_centers', data=surface_mesh_centers, \n",
    "                            chunks=(10000, 3))\n",
    "        root.create_array('surface_normals', data=surface_normals, \n",
    "                            chunks=(10000, 3))\n",
    "        root.create_array('surface_fields', data=surface_fields, \n",
    "                            chunks=(10000, 4))\n",
    "        \n",
    "        # STL data is (N_cells, ...)\n",
    "        root.create_array('stl_areas', data=stl_areas, \n",
    "                            chunks=(10000,))\n",
    "        root.create_array('stl_centers', data=stl_centers, \n",
    "                            chunks=(10000, 3))\n",
    "        \n",
    "        # Scalar parameters\n",
    "        root.create_array('air_density', data=np.array([density], dtype=np.float32))\n",
    "        root.create_array('stream_velocity', data=np.array([velocity], dtype=np.float32))\n",
    "        \n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {vtp_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Main Preprocessing Loop\n",
    "print(\"--- STAGE 1: PREPROCESSING ---\")\n",
    "\n",
    "# Find all VTP/STL files\n",
    "vtp_files = sorted(glob.glob(os.path.join(VTP_DIR, \"*.vtp\")))\n",
    "stl_files = sorted(glob.glob(os.path.join(STL_DIR, \"*.stl\")))\n",
    "\n",
    "if len(vtp_files) == 0 or len(vtp_files) != len(stl_files):\n",
    "    raise FileNotFoundError(f\"Found {len(vtp_files)} VTP files and {len(stl_files)} STL files. \"\n",
    "                           \"Paths may be wrong, or counts mismatch.\")\n",
    "\n",
    "print(f\"Found {len(vtp_files)} matching VTP/STL simulation pairs.\")\n",
    "\n",
    "# Zip them together and shuffle\n",
    "file_pairs = list(zip(stl_files, vtp_files))\n",
    "random.shuffle(file_pairs)\n",
    "\n",
    "# Split into train and val\n",
    "split_index = int(len(file_pairs) * TRAIN_SPLIT_RATIO)\n",
    "train_files = file_pairs[:split_index]\n",
    "val_files = file_pairs[split_index:]\n",
    "\n",
    "print(f\"Splitting data: {len(train_files)} training, {len(val_files)} validation.\")\n",
    "\n",
    "# Process Training Files\n",
    "print(\"\\nProcessing TRAINING files...\")\n",
    "for i, (stl_path, vtp_path) in enumerate(tqdm(train_files, desc=\"Train Convert\")):\n",
    "    # sim_000.stl -> sim_000.zarr\n",
    "    file_name = os.path.basename(stl_path).replace(\".stl\", \".zarr\")\n",
    "    zarr_path = os.path.join(TRAIN_ZARR_DIR, file_name)\n",
    "    process_one_simulation(stl_path, vtp_path, zarr_path, AIR_DENSITY, STREAM_VELOCITY)\n",
    "\n",
    "# Process Validation Files\n",
    "print(\"\\nProcessing VALIDATION files...\")\n",
    "for i, (stl_path, vtp_path) in enumerate(tqdm(val_files, desc=\"Val Convert\")):\n",
    "    file_name = os.path.basename(stl_path).replace(\".stl\", \".zarr\")\n",
    "    zarr_path = os.path.join(VAL_ZARR_DIR, file_name)\n",
    "    process_one_simulation(stl_path, vtp_path, zarr_path, AIR_DENSITY, STREAM_VELOCITY)\n",
    "    \n",
    "print(\"\\n--- STAGE 1: COMPLETE ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f385e23c-f160-447b-8caa-b1547ac916b0",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "This code block performs the critical \"pre-flight check\" of **normalizing** our training data. It has one essential goal: to create a file named `surface_fields_normalization.npz` that the training script in Part 4 will use.\n",
    "\n",
    "Here’s a breakdown of what's happening:\n",
    "\n",
    "* **The \"Why\":** Our target data, `surface_fields`, contains both **pressure** (e.g., values from -500 to +1000) and **wall shear stress** (e.g., values from 0.01 to 5.0). If we feed these raw numbers to the model, the \"loud\" pressure will dominate the loss calculation, and the model will learn to ignore the \"quiet\" wall shear stress. This script calculates the **`mean`** and **`std`** (standard deviation) for each of our 4 output channels (p, wss_x, wss_y, wss_z) so the training script can \"z-score\" them, putting them all on a similar scale.\n",
    "\n",
    "* **`os.environ` Fix:**\n",
    "    * This block of code starting with `import os` is a **critical fix** to run this pipeline in a notebook.\n",
    "    * The `DomainParallelZarrDataset` is \"distributed-aware\" and tries to connect to a multi-GPU cluster by default. This was causing your `DistStoreError` (the 10-minute timeout).\n",
    "    * These lines **force** PyTorch's distributed library into **single-process mode** by manually setting the environment variables `WORLD_SIZE=\"1\"` and `RANK=\"0\"`, telling it \"I am the only GPU, don't wait for others.\"\n",
    "\n",
    "* **`DomainParallelZarrDataset`:**\n",
    "    * Here, we initialize the datapipe from `datapipe.py`.\n",
    "    * Crucially, we only pass it the `TRAIN_ZARR_DIR`. We **must** compute our statistics *only* on the training data. Using the validation data would \"leak\" information to the model, invalidating our validation results.\n",
    "    * We also specify `keys_to_read=['surface_fields']` to make it extra-fast, as that's the only array we care about for this step.\n",
    "\n",
    "* **`compute_mean_std_min_max_with_progress`:**\n",
    "    * We have copied the logic from the `compute_normalizations.py` script directly into this notebook.\n",
    "    * **Why?** The only reason is to add the `from tqdm import tqdm` wrapper around the `for` loop. This gives us the **progress bar** (`Computing Norms: 100%|...`) so we can see that the script is working and not frozen.\n",
    "\n",
    "* **The `for` loop (Welford's Algorithm):**\n",
    "    * This loop iterates through all 400+ files in your training set.\n",
    "    * It **does not** load them all into memory at once, which would crash your machine.\n",
    "    * Instead, it uses **Welford's algorithm**: it loads one file, calculates its *local* statistics (`batch_mean`, `batch_M2`), and then uses those to update the *global* `mean` and `M2` (sum of squares of differences) accumulators. This is a highly efficient and stable way to get the statistics for a massive dataset, one file at a time.\n",
    "\n",
    "* **`np.savez`:**\n",
    "    * This is the final, most important step.\n",
    "    * After the loop finishes, the function calculates the final `std` from the `M2` accumulator.\n",
    "    * `np.savez` then saves the computed `mean`, `std`, `min_val`, and `max_val` tensors into a single compressed file: `surface_fields_normalization.npz`.\n",
    "    * This file is the **key** that Stage 3 (Training) needs to work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dd1bd77-faa8-482b-afd9-6f1dd8bcd9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STAGE 2: COMPUTING NORMALIZATION ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Norms: 100%|██████████| 367/367 [00:02<00:00, 162.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed Mean: [-1.7354057e+02 -2.4689510e+00 -2.1376118e-01 -4.7654234e-02]\n",
      "Computed Std: [346.60336     2.1168454   0.8955206   0.9657397]\n",
      "\n",
      "Successfully saved normalization file to: surface_fields_normalization.npz\n",
      "--- STAGE 2: COMPLETE ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# Normalization\n",
    "# ===============================================================\n",
    "print(\"--- STAGE 2: COMPUTING NORMALIZATION ---\")\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"RANK\"] = f\"0\"\n",
    "os.environ[\"WORLD_SIZE\"] = f\"1\"\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"MASTER_PORT\"] = str(12356)\n",
    "os.environ[\"LOCAL_RANK\"] = f\"0\"\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# We only need a *minimal* datapipe for this step.\n",
    "norm_dataset = DomainParallelZarrDataset(\n",
    "    data_path=TRAIN_ZARR_DIR,\n",
    "    device_mesh=None,\n",
    "    placements=None,\n",
    "    max_workers=4,\n",
    "    pin_memory=False,\n",
    "    keys_to_read=['surface_fields'],\n",
    "    large_keys=['surface_fields'],\n",
    ")\n",
    "\n",
    "# compute_mean_std_min_max_with_progress\n",
    "# We re-implement the function from utils/compute_normalizations.py\n",
    "# here so we can add a tqdm progress bar.\n",
    "\n",
    "def compute_mean_std_min_max_with_progress(\n",
    "    dataset: DomainParallelZarrDataset, field_key: str\n",
    ") -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Compute the mean, standard deviation, minimum, and maximum for a specified field\n",
    "    across all samples in a dataset. (with tqdm progress bar)\n",
    "    \"\"\"\n",
    "    N = 0  # Total number of elements processed\n",
    "    mean = None\n",
    "    M2 = None  # Sum of squares of differences from the current mean\n",
    "    min_val = None\n",
    "    max_val = None\n",
    "\n",
    "    for i in tqdm(range(len(dataset)), desc=\"Computing Norms\"):\n",
    "        data = dataset[i][field_key]\n",
    "        if mean is None:\n",
    "            # Initialize accumulators based on the shape of the data\n",
    "            mean = torch.zeros(data.shape[-1], device=data.device)\n",
    "            M2 = torch.zeros(data.shape[-1], device=data.device)\n",
    "            min_val = torch.full((data.shape[-1],), float(\"inf\"), device=data.device)\n",
    "            max_val = torch.full((data.shape[-1],), float(\"-inf\"), device=data.device)\n",
    "        \n",
    "        # This part of the original script was incorrect, n was unused\n",
    "        # n = data.shape[1]\n",
    "        # N += n \n",
    "        \n",
    "        # Compute batch statistics\n",
    "        batch_mean = data.mean(axis=(0, 1))\n",
    "        batch_M2 = ((data - batch_mean) ** 2).sum(axis=(0, 1))\n",
    "        batch_n = data.shape[1] # This is the correct n\n",
    "\n",
    "        # Update min/max\n",
    "        batch_min = data.amin(dim=(0, 1))\n",
    "        batch_max = data.amax(dim=(0, 1))\n",
    "        min_val = torch.minimum(min_val, batch_min)\n",
    "        max_val = torch.maximum(max_val, batch_max)\n",
    "\n",
    "        # Update running mean and M2 (Welford's algorithm)\n",
    "        delta = batch_mean - mean\n",
    "        N_old = N # Store old N\n",
    "        N += batch_n\n",
    "        mean = mean + delta * (batch_n / N)\n",
    "        M2 = M2 + batch_M2 + delta**2 * (batch_n * N_old) / N # Corrected Welford's\n",
    "\n",
    "    var = M2 / (N - 1)\n",
    "    std = torch.sqrt(var)\n",
    "    return mean, std, min_val, max_val\n",
    "\n",
    "# This runs our new function with the progress bar\n",
    "mean, std, min_val, max_val = compute_mean_std_min_max_with_progress(norm_dataset, 'surface_fields')\n",
    "\n",
    "print(f\"Computed Mean: {mean.cpu().numpy()}\")\n",
    "print(f\"Computed Std: {std.cpu().numpy()}\")\n",
    "\n",
    "# Save the statistics to the .npz file that train.py expects\n",
    "norm_file_path = \"surface_fields_normalization.npz\"\n",
    "np.savez(\n",
    "    norm_file_path,\n",
    "    mean=mean.cpu().numpy(),\n",
    "    std=std.cpu().numpy(),\n",
    "    min=min_val.cpu().numpy(),\n",
    "    max=max_val.cpu().numpy(),\n",
    ")\n",
    "\n",
    "print(f\"\\nSuccessfully saved normalization file to: {norm_file_path}\")\n",
    "print(\"--- STAGE 2: COMPLETE ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cd0356-b0f0-4022-b92d-0de12f5ee890",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "This is the final and most important part of our series. This block assembles all our previous work—our data pipeline, our preprocessing logic, and our `Transolver` model—into a complete, runnable training loop. We are adapting the `train.py` script to run in a notebook, which makes it much easier to see what's happening step-by-step.\n",
    "\n",
    "Here is a breakdown of the code:\n",
    "\n",
    "* **1. Setup Environment (Single GPU):**\n",
    "    * **`MockDistributedManager`:** This is a small helper \"class\" we create to trick the Physics-NEMO scripts. The scripts (like `datapipe.py`) are \"distributed-aware\" and expect to be in a multi-GPU cluster. This mock class tells them, \"You are in a distributed setup, but the `WORLD_SIZE` is 1,\" which forces them to run in a simple, single-process mode that works in this notebook.\n",
    "    * **`print()` vs. `logger`:** We are using standard `print()` statements instead of the `logger` because `logger` often doesn't display output correctly inside a Jupyter notebook.\n",
    "\n",
    "* **2. Load Normalization Factors:**\n",
    "    * This is the first \"payoff\" step. It loads the `surface_fields_normalization.npz` file that we created in **Stage 2**.\n",
    "    * It converts the `mean` and `std` arrays from NumPy into PyTorch Tensors and moves them to the GPU (`.to(dist_manager.device)`), so they are ready to be used by the loss and metrics functions.\n",
    "\n",
    "* **3. Create Datasets:**\n",
    "    * This is where we initialize our `DomainParallelZarrDataset` (from `datapipe.py`).\n",
    "    * We create one for our `TRAIN_ZARR_DIR` and one for our `VAL_ZARR_DIR`.\n",
    "    * We pass the `data_keys` list to tell the datapipe exactly which arrays it *must* load from each Zarr file (e.g., `surface_mesh_centers`, `surface_fields`).\n",
    "\n",
    "* **4. Instantiate Model:**\n",
    "    * This is the moment we **create the `Transolver` model** from the `physicsnemo` library.\n",
    "    * We pass in all the parameters we defined in **Part 1**.\n",
    "    * `functional_dim=2`: Tells the model to expect 2 \"functional\" inputs (air_density, stream_velocity).\n",
    "    * `embedding_dim=6`: Tells the model to expect 6 \"geometric\" inputs (the 3D coords + 3D normals).\n",
    "    * `out_dim=4`: Tells the model it must predict 4 output channels (pressure, wss_x, wss_y, wss_z).\n",
    "    * `n_hidden`, `n_layers`, `n_head`, `slice_num`: These are the core architecture hyperparameters (hidden dimension, number of layers, number of heads, and number of slices) that define our specific model.\n",
    "    * `model.to(...)` moves the model's millions of parameters to the GPU.\n",
    "    * `torchinfo.summary(...)` prints a summary of the model so we can confirm its architecture and see the total parameter count.\n",
    "\n",
    "* **5. Setup Optimizer, Scheduler, Scaler:**\n",
    "    * This creates the three essential tools for modern training:\n",
    "    * **Optimizer (`torch.optim.AdamW`):** The engine that updates the model's weights. We use AdamW, as specified in your config file.\n",
    "    * **Scheduler (`torch.optim.lr_scheduler.OneCycleLR`):** This is a \"learning rate\" manager. Instead of a fixed learning rate, it will automatically warm up the rate, peak, and then cool it down over the 5 epochs, which often leads to faster and more stable training.\n",
    "    * **Scaler (`GradScaler`):** A helper for mixed-precision (`float16`) training. It scales the loss to prevent numerical errors (underflow) during the backward pass.\n",
    "\n",
    "* **6. Helper functions (`get_autocast_context`):**\n",
    "    * This is a helper function from the original `train.py` script.\n",
    "    * It returns a PyTorch `autocast` context if `PRECISION` is set to \"float16\", or an \"empty\" `nullcontext` if we're using \"float32\". This allows us to easily switch between precision modes.\n",
    "\n",
    "* **7. The \"forward_pass\" function:**\n",
    "    * This is a custom function we made for this notebook. It cleanly bundles all the logic for a **single training step**:\n",
    "    1.  Moves the `batch` of data (loaded by the datapipe) to the GPU.\n",
    "    2.  Calls `preprocess_surface_data` to normalize the targets and center the geometry.\n",
    "    3.  Applies our bug fixes: `.squeeze(1)` for the `features` shape and `.to(target_dtype)` to cast the data from `float64` to `float32`.\n",
    "    4.  Calls `downsample_surface` (with `num_keep=-1` to use the full, un-downsampled mesh).\n",
    "    5.  Runs the **model prediction** (`outputs = model(...)`) inside the `autocast` context.\n",
    "    6.  Calls `loss_fn` to get the error score.\n",
    "    7.  Calls `metrics_fn` to get the human-readable L2 errors.\n",
    "\n",
    "* **8. The Main Training Loop:**\n",
    "    * This is the \"manager\" that runs the entire process.\n",
    "    * `train_indices`, `val_indices`: Creates lists of indices (e.g., `[0, 1, ..., 456]`). This is our fix to iterate over the `DomainParallelZarrDataset` correctly without a `DataLoader`.\n",
    "    * **`for epoch in ...`**: The outer loop that runs 5 times.\n",
    "    * **Training (`model.train()`)**:\n",
    "        * `random.shuffle(train_indices)`: **Crucial for learning.** It shuffles the training data order every epoch so the model doesn't just memorize a sequence.\n",
    "        * `for i in tqdm(...)`: The inner loop that iterates through all 457 training files.\n",
    "        * `batch = train_dataset[i]`: Loads one simulation file.\n",
    "        * `loss, _ = notebook_forward_pass(...)`: Runs the model and gets the loss.\n",
    "        * `loss.backward()`: **This is the \"learning\" step.** PyTorch calculates how every weight in the model contributed to the error.\n",
    "        * `optimizer.step()`: **This is the \"update\" step.** The AdamW optimizer uses the calculated gradients to update all the model's weights.\n",
    "        * `scheduler.step()`: Updates the learning rate.\n",
    "    * **Validation (`model.eval()`)**:\n",
    "        * `with torch.no_grad()`: **Very important.** This tells PyTorch to *turn off* all gradient calculations, making this step much faster and use less memory since we are only testing, not learning.\n",
    "        * It loops over the 51 *validation* files and calculates the `avg_val_loss` and the average of all metrics (like `l2_pressure`).\n",
    "    * **Logging & Checkpointing**:\n",
    "        * At the end of each epoch, it `print`s the summary (Train Loss, Val Loss) and the table of validation metrics.\n",
    "        * `torch.save(...)`: It saves a `.pt` checkpoint file containing all the model's weights, so we can resume training later or use this model for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53802083-2b96-47e5-9961-47e289d85219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STAGE 3: TRAINING ---\n",
      "Running in single-process mode on: cuda\n",
      "Loaded normalization factors.\n",
      "Loaded 367 training samples and 41 val samples.\n",
      "\n",
      "=====================================================================================\n",
      "Layer (type:depth-idx)                                       Param #\n",
      "=====================================================================================\n",
      "Transolver                                                   --\n",
      "├─MLP: 1-1                                                   --\n",
      "│    └─GELU: 2-1                                             --\n",
      "│    └─Linear: 2-2                                           1,152\n",
      "│    └─Linear: 2-3                                           8,256\n",
      "│    └─ModuleList: 2-4                                       --\n",
      "├─ModuleList: 1-2                                            --\n",
      "│    └─Transolver_block: 2-5                                 --\n",
      "│    │    └─LayerNorm: 3-1                                   128\n",
      "│    │    └─PhysicsAttentionIrregularMesh: 3-2               13,524\n",
      "│    │    └─LayerNormMLP: 3-3                                33,216\n",
      "│    └─Transolver_block: 2-6                                 --\n",
      "│    │    └─LayerNorm: 3-4                                   128\n",
      "│    │    └─PhysicsAttentionIrregularMesh: 3-5               13,524\n",
      "│    │    └─LayerNormMLP: 3-6                                33,216\n",
      "│    └─Transolver_block: 2-7                                 --\n",
      "│    │    └─LayerNorm: 3-7                                   128\n",
      "│    │    └─PhysicsAttentionIrregularMesh: 3-8               13,524\n",
      "│    │    └─LayerNormMLP: 3-9                                33,216\n",
      "│    └─Transolver_block: 2-8                                 --\n",
      "│    │    └─LayerNorm: 3-10                                  128\n",
      "│    │    └─PhysicsAttentionIrregularMesh: 3-11              13,524\n",
      "│    │    └─LayerNormMLP: 3-12                               33,216\n",
      "│    │    └─LayerNormLinear: 3-13                            388\n",
      "=====================================================================================\n",
      "Total params: 197,268\n",
      "Trainable params: 197,268\n",
      "Non-trainable params: 0\n",
      "=====================================================================================\n",
      "Model Parameters: 197,268\n",
      "--- STARTING TRAINING for 5 epochs ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 [Train]: 100%|██████████| 367/367 [00:33<00:00, 10.85it/s]\n",
      "Epoch 1/5 [Val]: 100%|██████████| 41/41 [00:01<00:00, 28.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 1 Summary ---\n",
      "Avg Train Loss: 0.955093\n",
      "Avg Val Loss: 0.784624\n",
      "\n",
      "Validation Metrics:\n",
      "+-------------+--------------------+\n",
      "|   Metric    |       Value        |\n",
      "+-------------+--------------------+\n",
      "| l2_pressure | 1.0576273072056654 |\n",
      "| l2_shear_x  | 0.9159264099307176 |\n",
      "| l2_shear_y  | 0.8418318760104295 |\n",
      "| l2_shear_z  | 0.8658466906082339 |\n",
      "+-------------+--------------------+\n",
      "\n",
      "Saved checkpoint to epoch_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 [Train]: 100%|██████████| 367/367 [00:17<00:00, 20.42it/s]\n",
      "Epoch 2/5 [Val]: 100%|██████████| 41/41 [00:01<00:00, 26.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 2 Summary ---\n",
      "Avg Train Loss: 0.640705\n",
      "Avg Val Loss: 0.535185\n",
      "\n",
      "Validation Metrics:\n",
      "+-------------+--------------------+\n",
      "|   Metric    |       Value        |\n",
      "+-------------+--------------------+\n",
      "| l2_pressure | 0.8499916392128642 |\n",
      "| l2_shear_x  | 0.7555469354478325 |\n",
      "| l2_shear_y  | 0.6138811874680403 |\n",
      "| l2_shear_z  | 0.6324540834601332 |\n",
      "+-------------+--------------------+\n",
      "\n",
      "Saved checkpoint to epoch_2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 [Train]: 100%|██████████| 367/367 [00:17<00:00, 20.49it/s]\n",
      "Epoch 3/5 [Val]: 100%|██████████| 41/41 [00:01<00:00, 28.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 3 Summary ---\n",
      "Avg Train Loss: 0.518996\n",
      "Avg Val Loss: 0.480435\n",
      "\n",
      "Validation Metrics:\n",
      "+-------------+--------------------+\n",
      "|   Metric    |       Value        |\n",
      "+-------------+--------------------+\n",
      "| l2_pressure | 0.7825920036653193 |\n",
      "| l2_shear_x  | 0.7073652758830931 |\n",
      "| l2_shear_y  | 0.6021456158742672 |\n",
      "| l2_shear_z  | 0.636372776293173  |\n",
      "+-------------+--------------------+\n",
      "\n",
      "Saved checkpoint to epoch_3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 [Train]: 100%|██████████| 367/367 [00:18<00:00, 20.27it/s]\n",
      "Epoch 4/5 [Val]: 100%|██████████| 41/41 [00:01<00:00, 28.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 4 Summary ---\n",
      "Avg Train Loss: 0.479564\n",
      "Avg Val Loss: 0.464073\n",
      "\n",
      "Validation Metrics:\n",
      "+-------------+--------------------+\n",
      "|   Metric    |       Value        |\n",
      "+-------------+--------------------+\n",
      "| l2_pressure | 0.7980172241606364 |\n",
      "| l2_shear_x  | 0.6896794191220912 |\n",
      "| l2_shear_y  | 0.6064107134574797 |\n",
      "| l2_shear_z  | 0.6471653696967334 |\n",
      "+-------------+--------------------+\n",
      "\n",
      "Saved checkpoint to epoch_4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 [Train]: 100%|██████████| 367/367 [00:18<00:00, 19.53it/s]\n",
      "Epoch 5/5 [Val]: 100%|██████████| 41/41 [00:01<00:00, 27.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 5 Summary ---\n",
      "Avg Train Loss: 0.467872\n",
      "Avg Val Loss: 0.458189\n",
      "\n",
      "Validation Metrics:\n",
      "+-------------+--------------------+\n",
      "|   Metric    |       Value        |\n",
      "+-------------+--------------------+\n",
      "| l2_pressure | 0.7540002385290657 |\n",
      "| l2_shear_x  | 0.6556884639146852 |\n",
      "| l2_shear_y  | 0.6070183064879441 |\n",
      "| l2_shear_z  | 0.6404399472038921 |\n",
      "+-------------+--------------------+\n",
      "\n",
      "Saved checkpoint to epoch_5.pt\n",
      "--- STAGE 3: COMPLETE ---\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# Training the Model\n",
    "# ===============================================================\n",
    "\n",
    "print(\"--- STAGE 3: TRAINING ---\")\n",
    "\n",
    "# 1. Setup Environment (Single GPU)\n",
    "# We will create a \"mock\" DistributedManager for this notebook\n",
    "# This mimics the real one but runs as a single process.\n",
    "class MockDistributedManager:\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.world_size = 1\n",
    "        self.rank = 0\n",
    "        self.local_rank = 0\n",
    "        self.cuda = torch.cuda.is_available()\n",
    "\n",
    "dist_manager = MockDistributedManager()\n",
    "\n",
    "print(f\"Running in single-process mode on: {dist_manager.device}\")\n",
    "\n",
    "# 2. Load Normalization Factors\n",
    "try:\n",
    "    norm_data = np.load(norm_file_path)\n",
    "    norm_factors = {\n",
    "        \"mean\": torch.from_numpy(norm_data[\"mean\"]).to(dist_manager.device),\n",
    "        \"std\": torch.from_numpy(norm_data[\"std\"]).to(dist_manager.device),\n",
    "    }\n",
    "    print(\"Loaded normalization factors.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"FATAL: {norm_file_path} not found. Did Stage 2 fail?\")\n",
    "    raise\n",
    "\n",
    "# 3. Create Datasets\n",
    "data_keys = [\n",
    "    'surface_mesh_centers', 'surface_normals', 'surface_fields', \n",
    "    'stl_areas', 'stl_centers', 'air_density', 'stream_velocity'\n",
    "]\n",
    "large_keys = [\n",
    "    'surface_mesh_centers', 'surface_normals', 'surface_fields', \n",
    "    'stl_areas', 'stl_centers'\n",
    "]\n",
    "\n",
    "train_dataset = DomainParallelZarrDataset(\n",
    "    data_path=TRAIN_ZARR_DIR, device_mesh=None, placements=None,\n",
    "    keys_to_read=data_keys, large_keys=large_keys\n",
    ")\n",
    "val_dataset = DomainParallelZarrDataset(\n",
    "    data_path=VAL_ZARR_DIR, device_mesh=None, placements=None,\n",
    "    keys_to_read=data_keys, large_keys=large_keys\n",
    ")\n",
    "print(f\"Loaded {len(train_dataset)} training samples and {len(val_dataset)} val samples.\")\n",
    "\n",
    "# 4. Instantiate Model\n",
    "# We use the parameters from Part 1\n",
    "model = Transolver(\n",
    "    functional_dim=2,  # air_density + stream_velocity\n",
    "    embedding_dim=6,   # mesh_centers (3) + normals (3)\n",
    "    out_dim=4,         # pressure (1) + wallShearStress (3)\n",
    "    n_hidden=C_MODEL,      # <-- Corrected from latent_dim\n",
    "    n_layers=NUM_LAYERS,   # <-- Corrected from num_layers\n",
    "    n_head=NUM_HEADS,      # <-- Corrected from num_heads\n",
    "    slice_num=M_SLICES     # <-- Corrected from slice_size\n",
    "    # We will use the other defaults from the config (e.g., act='gelu')\n",
    ")\n",
    "model.to(dist_manager.device)\n",
    "\n",
    "# Set the device for torchinfo\n",
    "model_summary = torchinfo.summary(model, verbose=0)\n",
    "print(f\"\\n{model_summary}\")\n",
    "print(f\"Model Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# 5. Setup Optimizer, Scheduler, Scaler\n",
    "# Use AdamW as specified in the config\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=LEARNING_RATE, \n",
    "    weight_decay=1.0e-4, \n",
    "    betas=(0.9, 0.999), \n",
    "    eps=1.0e-8\n",
    ")\n",
    "\n",
    "# We must use `__len__` on the dataset, not the sampler, for single-GPU\n",
    "total_steps = len(train_dataset) * NUM_EPOCHS\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, \n",
    "    max_lr=LEARNING_RATE,\n",
    "    total_steps=total_steps,\n",
    "    pct_start=0.02,\n",
    "    anneal_strategy='cos',\n",
    "    div_factor=1e2,\n",
    "    final_div_factor=1e4 # From config scheduler.params\n",
    ")\n",
    "scaler = GradScaler() if PRECISION == \"float16\" else None\n",
    "\n",
    "\n",
    "# 6. Helper functions (get_autocast_context)\n",
    "from contextlib import nullcontext\n",
    "\n",
    "def get_autocast_context(precision: str) -> nullcontext:\n",
    "    if precision == \"float16\":\n",
    "        return autocast(\"cuda\", dtype=torch.float16)\n",
    "    elif precision == \"bfloat16\":\n",
    "        return autocast(\"cuda\", dtype=torch.bfloat16)\n",
    "    # Not including FP8 for this notebook\n",
    "    else:\n",
    "        return nullcontext()\n",
    "        \n",
    "\n",
    "# 7. The \"forward_pass\" function\n",
    "def notebook_forward_pass(batch, model, norm_factors):\n",
    "    \n",
    "    # Move the batch to the GPU\n",
    "    for key, value in batch.items():\n",
    "        batch[key] = value.to(dist_manager.device)\n",
    "\n",
    "    # 1. Preprocess\n",
    "    # 'others' will contain stream_velocity, etc. for metrics\n",
    "    features, embeddings, targets, others = preprocess_surface_data(batch, norm_factors)\n",
    "    \n",
    "    # Squeeze features from (1, 1, 2) to (1, 2)\n",
    "    if features.dim() == 3 and features.shape[1] == 1:\n",
    "        features = features.squeeze(1) \n",
    "\n",
    "    # 2. Downsample (use -1 for full resolution)\n",
    "    features, embeddings, targets = downsample_surface(features, embeddings, targets, num_keep=-1)\n",
    "\n",
    "    # 3. Cast precisions (from float64 to float32/16)\n",
    "    if PRECISION == \"float16\":\n",
    "        target_dtype = torch.float16\n",
    "    elif PRECISION == \"bfloat16\":\n",
    "        target_dtype = torch.bfloat16\n",
    "    else:\n",
    "        target_dtype = torch.float32 # Default to float32\n",
    "    \n",
    "    features = features.to(target_dtype)\n",
    "    embeddings = embeddings.to(target_dtype)\n",
    "\n",
    "    # 4. Run model with autocast\n",
    "    with get_autocast_context(PRECISION):\n",
    "        outputs = model(features, embeddings)\n",
    "        loss = loss_fn(outputs, targets, mode=\"surface\")\n",
    "\n",
    "    # 5. Compute metrics\n",
    "    metrics = metrics_fn(outputs, targets, others, dist_manager, \"surface\", norm_factors)\n",
    "    return loss, metrics\n",
    "\n",
    "# 8. The Main Training Loop\n",
    "print(f\"--- STARTING TRAINING for {NUM_EPOCHS} epochs ---\")\n",
    "\n",
    "# Create samplers (lists of indices)\n",
    "train_indices = list(range(len(train_dataset)))\n",
    "val_indices = list(range(len(val_dataset)))\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    # --- TRAINING EPOCH ---\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    \n",
    "    # Shuffle indices and iterate\n",
    "    random.shuffle(train_indices)\n",
    "    for i in tqdm(train_indices, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Train]\"):\n",
    "        # Get batch by indexing the dataset\n",
    "        batch = train_dataset[i]\n",
    "        \n",
    "        loss, _ = notebook_forward_pass(batch, model, norm_factors)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        if scaler:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        scheduler.step()\n",
    "        \n",
    "        # Detach loss to fix UserWarning\n",
    "        total_train_loss += loss.detach().item()\n",
    "        \n",
    "    avg_train_loss = total_train_loss / len(train_dataset)\n",
    "    \n",
    "    # VALIDATION EPOCH \n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    all_val_metrics = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Iterate over validation indices\n",
    "        for i in tqdm(val_indices, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Val]\"):\n",
    "            # Get batch by indexing the dataset\n",
    "            batch = val_dataset[i]\n",
    "            \n",
    "            loss, metrics = notebook_forward_pass(batch, model, norm_factors)\n",
    "            \n",
    "            # Increment total_val_loss, not total_train_loss\n",
    "            total_val_loss += loss.detach().item()\n",
    "            \n",
    "            # Aggregate metrics\n",
    "            for k, v in metrics.items():\n",
    "                if k not in all_val_metrics:\n",
    "                    all_val_metrics[k] = []\n",
    "                all_val_metrics[k].append(v.item())\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_dataset)\n",
    "    avg_val_metrics = {k: np.mean(v) for k, v in all_val_metrics.items()}\n",
    "    \n",
    "    # LOGGING\n",
    "    print(f\"--- Epoch {epoch+1} Summary ---\")\n",
    "    print(f\"Avg Train Loss: {avg_train_loss:.6f}\")\n",
    "    print(f\"Avg Val Loss: {avg_val_loss:.6f}\")\n",
    "    \n",
    "    # Print metrics table\n",
    "    metrics_table = tabulate(\n",
    "        avg_val_metrics.items(), headers=[\"Metric\", \"Value\"], tablefmt=\"pretty\"\n",
    "    )\n",
    "    print(f\"\\nValidation Metrics:\\n{metrics_table}\\n\")\n",
    "    \n",
    "    # --- Save Checkpoint ---\n",
    "    if dist_manager.rank == 0:\n",
    "        ckpt_name = f\"epoch_{epoch+1}.pt\"\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'loss': avg_val_loss,\n",
    "        }, os.path.join(CHECKPOINT_PATH, ckpt_name))\n",
    "        print(f\"Saved checkpoint to {ckpt_name}\")\n",
    "\n",
    "print(\"--- STAGE 3: COMPLETE ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ec4520-f141-42af-8847-0259d6e01507",
   "metadata": {},
   "source": [
    "### Explanation of the Model Summary\n",
    "\n",
    "This output from `torchinfo.summary` gives us a \"blueprint\" of the `Transolver` model we just trained. It shows every layer, how they are nested, and how many parameters each one has. This is the model that corresponds to the 5-epoch training run we just completed.\n",
    "\n",
    "Let's break down the key components:\n",
    "\n",
    "* **`Transolver` (Total params: 197,268):** This is the top-level \"container\" for our entire model. The total number of parameters (197,268) is the total number of \"dials\" or \"weights\" the optimizer had to tune during training.\n",
    "\n",
    "* **`├─MLP: 1-1` (9,408 params):** This is the **Input Preprocessor**.\n",
    "    * This Multi-Layer Perceptron (MLP) is the first thing that touches our data.\n",
    "    * Its job is to take our 8 input features (`functional_dim=2` + `embedding_dim=6`) and \"embed\" them into the model's internal processing dimension (the `n_hidden=64` dimension).\n",
    "\n",
    "* **`├─ModuleList: 1-2` (187,472 params):** This is the **main body** of the model.\n",
    "    * This is a list containing our **4 `Transolver_block`s** (because we set `NUM_LAYERS = 4` in our config).\n",
    "    * This stack of 4 blocks is what does the deep learning, with each layer refining the solution.\n",
    "\n",
    "* **`└─Transolver_block: 2-5` (46,868 params):** This is one of the four identical blocks. It is the \"engine\" of the model and contains the two standard parts of any Transformer:\n",
    "    1.  **`└─PhysicsAttentionIrregularMesh: 3-2` (13,524 params):** This is the **core of Transolver**. It's the \"smart\" attention layer we studied in Notebook 2. It performs the 4-step *Slice, Aggregate, $O(M^2)$ Attend, Deslice* process.\n",
    "    2.  **`└─LayerNormMLP: 3-3` (33,216 params):** This is the **\"Feed-Forward\" network**. After the attention layer gathers information, this MLP \"thinks\" about that information and processes it before passing it to the next block.\n",
    "\n",
    "* **`└─LayerNormLinear: 3-13` (388 params):** This is the **Output Head**.\n",
    "    * After the data has passed through all 4 `Transolver_block`s, this final layer takes the processed 64-dimensional feature vector.\n",
    "    * It applies one last `LayerNorm` and then uses a `Linear` layer to project those 64 features down to our **4 required output channels** (pressure, wss_x, wss_y, wss_z).\n",
    "    * The output of this layer is the model's final prediction.\n",
    "\n",
    "### The \"508 Sample\" Problem in Numbers\n",
    "\n",
    "This summary clearly shows our \"overfitting\" problem:\n",
    "* **Total Trainable Parameters:** 197,268\n",
    "* **Total Training Samples:** ~457\n",
    "\n",
    "We have almost 200,000 \"dials\" to tune, but we are only giving the model 457 examples to learn from. This is why the model can easily \"memorize\" the training data (leading to a low training loss) but fails to learn the *general physics*, which is why the validation loss will be much higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb981d23-aa29-4321-a59e-55bd30ea8e92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
