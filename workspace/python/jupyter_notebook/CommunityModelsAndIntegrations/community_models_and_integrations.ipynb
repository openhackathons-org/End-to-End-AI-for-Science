{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8005778a",
   "metadata": {},
   "source": [
    "# A Tutorial on Community Models and Integrations - PhysicsNeMo\n",
    "\n",
    "In this tutorial, PhysicsNeMo is used to expand the scope of community models and datasets, such as [The Well](https://github.com/PolymathicAI/the_well) by leveraging physics informed utilities, optimized model layers, and MLOps best practices. Specifically, the tutorial covers:\n",
    ". Loading and evaluating a checkpoint from The Well \n",
    ". How to use a pretrained checkpoint from The Well and run it as a PhysicsNeMo user\n",
    ". Training community models with PhysicsNeMo\n",
    ". Fine-tuning community models with PhysicsNeMo\n",
    ". Experimenting with different architectures, from the community and internal to PhysicsNeMo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa944336",
   "metadata": {},
   "source": [
    "## Loading Models and Data from The Well\n",
    "\n",
    "To begin, a model and dataset from The Well is selected for use throughout this example. For this example, the [Magnetohydrodynamics dataset](https://github.com/PolymathicAI/the_well/tree/master/datasets/MHD_64) is used. Magnetohydrodynamics (MHD), is the study of the dynamics of electrically conducting fluids such as plasmas.\n",
    "\n",
    "Note that any one of the models and dataset combinations may be selected.\n",
    "\n",
    "In addition to the data, a pre-trained model with a Tucker-Factorized Fourier Neural Operator (TFNO) architecture is used that will later be converted to PhysicsNeMo.\n",
    "\n",
    "The dataset streaming functionality from The Well can be utilized so that the dataset does not need to be downloaded locally. This requires accessing huggingface. Alternatively, the dataset can be downloaded which takes around 71GB of storage. The command to download all splits is `the-well-download --base-path path/to/base --dataset MHD_64`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affec459",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from the_well.benchmark.models import TFNO\n",
    "from torchinfo import summary\n",
    "\n",
    "# Load The Well model\n",
    "well_model = TFNO.from_pretrained(\"polymathic-ai/TFNO-MHD_64\")\n",
    "\n",
    "# Have a look at the model summary\n",
    "# Note that the order of layers does not represent order of execution\n",
    "summary(well_model, depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3381f0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from the_well.data import WellDataset\n",
    "from the_well.data.normalization import ZScoreNormalization\n",
    "\n",
    "# Enable streaming the dataset from HuggingFace or loading from local directory\n",
    "# The following line may take a couple of minutes to instantiate the datamodule\n",
    "dataset = WellDataset(\n",
    "    # well_base_path=\"hf://datasets/polymathic-ai/\",  # access from HF hub, or locally.\n",
    "    well_base_path=\"./TheWellMHDData/datasets\",\n",
    "    well_dataset_name=\"MHD_64\",\n",
    "    well_split_name=\"train\",\n",
    "    use_normalization=True,\n",
    "    normalization_type=ZScoreNormalization,\n",
    "    n_steps_input=4,\n",
    "    n_steps_output=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc7787d",
   "metadata": {},
   "source": [
    "With the dataset on hand, its features, shape and size can be explored. `The Well` provides a great script for this already, and is left to the reader to go through if desired. A summary is provided below, also available on the [dataset card](https://github.com/PolymathicAI/the_well/blob/master/datasets/MHD_64/README.md) online:\n",
    "\n",
    "**Dimension of discretized data:** 100 timesteps of 64 $\\times$ 64 $\\times$ 64 cubes.\n",
    "\n",
    "**Fields available in the data:** Density (scalar field), velocity (vector field), magnetic field (vector field).\n",
    "\n",
    "**Number of trajectories:** 10 Initial conditions x 10 combination of parameters = 100 trajectories.\n",
    "\n",
    "**Estimated size of the ensemble of all simulations:** 71.6 GB.\n",
    "\n",
    "**Grid type:** uniform grid, cartesian coordinates.\n",
    "\n",
    "**Initial conditions:** uniform IC.\n",
    "\n",
    "**Boundary conditions:** periodic boundary conditions.\n",
    "\n",
    "**Data are stored separated by ($\\Delta t$):** 0.01 (arbitrary units).\n",
    "\n",
    "**Total time range ($t\\_{min}$ to $t\\_{max}$):** $t\\_{min} = 0$, $t\\_{max} = 1$.\n",
    "\n",
    "**Spatial domain size ($L_x$, $L_y$, $L_z$):** dimensionless so 64 pixels.\n",
    "\n",
    "**Set of coefficients or non-dimensional parameters evaluated:** all combinations of $\\mathcal{M}_s=${0.5, 0.7, 1.5, 2.0 7.0} and $\\mathcal{M}_A =${0.7, 2.0}.\n",
    "\n",
    "**Approximate time and hardware used to generate the data:** Downsampled from `MHD_256` after applying ideal low-pass filter.\n",
    "\n",
    "**What phenomena of physical interest are catpured in the data:** MHD fluid flows in the compressible limit (sub and super sonic, sub and super Alfvenic).\n",
    "\n",
    "**How to evaluate a new simulator operating in this space:** Check metrics such as Power spectrum, two-points correlation function.\n",
    "\n",
    "Please cite the associated paper if you use this data in your research:\n",
    "\n",
    "```\n",
    "@article{burkhart2020catalogue,\n",
    "  title={The catalogue for astrophysical turbulence simulations (cats)},\n",
    "  author={Burkhart, B and Appel, SM and Bialy, S and Cho, J and Christensen, AJ and Collins, D and Federrath, Christoph and Fielding, DB and Finkbeiner, D and Hill, AS and others},\n",
    "  journal={The Astrophysical Journal},\n",
    "  volume={905},\n",
    "  number={1},\n",
    "  pages={14},\n",
    "  year={2020},\n",
    "  publisher={IOP Publishing}\n",
    "}\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee44dbf1",
   "metadata": {},
   "source": [
    "A single sample can be extracted and inspected. Some notes from `The Well` are shown below. More info on examining their data can be found in [this example](https://github.com/PolymathicAI/the_well/blob/master/docs/tutorials/dataset.ipynb):\n",
    "\n",
    "The most important elements are `input_fields` and `output_fields`. They represent the time-varying physical fields of the dynamical system and are generally the input and target of our models. For a dynamical system that has 2 spatial dimensions $x$ and $y$, `input_fields` would have a shape $(T_{in}, L_x, L_y, F)$ and `output_fields` would have a shape $(T_{out}, L_x, L_y, F)$. The number of input and output timesteps $T_{in}$ and $T_{out}$ are specified at the instantiation of the dataset with the arguments `n_steps_input` and `n_steps_output`. $L_x$ and $L_y$ are the lengths of the spatial dimensions. $F$ represents the number of physical fields, where vector fields $v = (v_x, v_y)$ and tensor fields $t = (t_{xx}, t_{xy}, t_{yx}, t_{yy})$ are flattened.\n",
    "\n",
    "Note that the dataset for MHD has three spatial dimensions, so an extra $z$ term will be included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ef513f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "sample = dataset[0]\n",
    "for k, v in sample.items():\n",
    "    print(f\"Key: {k.ljust(20)} Shape: {v.shape}\")\n",
    "\n",
    "print(f\"Field Names: {dataset.metadata.field_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef82c7da",
   "metadata": {},
   "source": [
    "Using the model summary from above and the order of operations in the [TFNO forward pass](https://github.com/neuraloperator/neuraloperator/blob/7bb578df787d6ca548b623b83f0601c98dc931fb/neuralop/models/fno.py#L337), the data is processed by:\n",
    "\n",
    "1. Applying optional positional encoding\n",
    "2. Sending inputs through a lifting layer to a high-dimensional latent space\n",
    "3. Applying optional domain padding to high-dimensional intermediate function representation\n",
    "4. Applying `n_layers` Fourier/TFNO layers in sequence (SpectralConvolution + skip connections, nonlinearity) \n",
    "5. If domain padding was applied, domain padding is removed\n",
    "6. Projection of intermediate function representation to the output channels\n",
    "\n",
    "\n",
    "This pretrained model is trained to predict the $T_{out} = 1$ next states given the $T_{in} = 4$ previous states. The input steps are concatenated along their channels, such that the model expects $T_{in} \\times F$ channels as input and $T_{out} \\times F$ channels as output. Because `WellDataset` is a PyTorch dataset, we can use it conveniently with PyTorch data-loaders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219d51d0-246b-4860-8b65-4e26792ba734",
   "metadata": {},
   "source": [
    "We can now evaluate and verify the performacne of the pretrained MHD model from `the_well`. To do this, we will utilize the _streaming_ data mode from `the_well`, and the utilities in the `Trainer` for validation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0915ac36-ea41-4821-b38e-1e874cb8da12",
   "metadata": {},
   "source": [
    "## Using The Well Data to Train PhysicsNeMo Models\n",
    "Now that the baseline model from The Well has been evaluated, lets look at training a similar architecture from scratch in `PhysicsNeMo`. In the `PhysicsNeMo` examples, there is a similar example of a Tensor-Factorized Fourier Neural Operator implementation that has been used in a related use case for incompressible magnetohydrodynamics in 2D. This model can be adapted for use in 3D with the dataset from The Well. This also serves as an example for how `PhysicsNeMo` can be used as a framework for building and testing model architectures that are not found in base model collection. For the remainder of this notebook, a boiler-plate `Trainer` class is implemented in `utils.py` that contains the core components needed for loading models, running training loops, saving checkpoints, evaluating models, etc. The method `setup_model` needs to be implemented in order to use a model from PhysicsNeMo. For completeness, the TFNO architecture from `PhysicsNeMo` is included in the `tfno` folder. Additionally, there is a config file in `config` folder that is used to initialize some parameters for our pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89719e84-8a2e-4056-8b5c-d0c07ed1c21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfd6ad7-efaf-4113-82f8-0f79d4e46b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "from hydra import compose, initialize\n",
    "from training_utils import Trainer\n",
    "\n",
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "\n",
    "initialize(version_base=None, config_path=\"./config\")\n",
    "cfg = compose(config_name=\"mhd_config.yaml\")\n",
    "cfg.output_dir = \"./TEMP\"\n",
    "cfg.train_params.epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09dee18-82aa-474f-8cb8-6d0de3cc5203",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHDTrainer(Trainer):\n",
    "    def setup_model(self):\n",
    "        \"\"\"Setup the TFNO model based in PhysicsNeMo. Input parameters are set and controlled by the yaml config.\"\"\"\n",
    "        from tfno.tfno import TFNO\n",
    "\n",
    "        self.model = TFNO(\n",
    "            in_channels=self.model_params.in_dim,\n",
    "            coord_features=self.model_params.coord_features,\n",
    "            out_channels=self.model_params.out_dim,\n",
    "            decoder_layers=self.model_params.decoder_layers,\n",
    "            decoder_layer_size=self.model_params.fc_dim,\n",
    "            dimension=self.model_params.dimension,\n",
    "            latent_channels=self.model_params.layers,\n",
    "            num_fno_layers=self.model_params.num_fno_layers,\n",
    "            num_fno_modes=self.model_params.modes,\n",
    "            padding=[\n",
    "                self.model_params.pad_z,\n",
    "                self.model_params.pad_y,\n",
    "                self.model_params.pad_x,\n",
    "            ],\n",
    "            rank=self.model_params.rank,\n",
    "            factorization=self.model_params.factorization,\n",
    "            fixed_rank_modes=self.model_params.fixed_rank_modes,\n",
    "            decomposition_kwargs=self.model_params.decomposition_kwargs,\n",
    "        ).to(self.dist.device)\n",
    "\n",
    "        print(summary(self.model, depth=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d158042-3c58-4079-8196-206ba038f9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.train_params.ckpt_path = \"./TEMP/checkpoints/pnm_model_well_data\"\n",
    "mhd_trainer = MHDTrainer(cfg)\n",
    "mhd_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc37d72-df24-4944-8d3a-37dcc0f65758",
   "metadata": {},
   "source": [
    "## Running Models From The Well in PhysicsNemo\n",
    "\n",
    "PhysicsNeMo offers great utility in running community models natively in PhysicsNeMo by way of a simple conversion. The documentation for this can be found [here](https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/api/physicsnemo.models.html#converting-pytorch-models-to-physicsnemo-models). The first process of the conversion is setting up the base PyTorch model as a PhysicsNeMo model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af4d77d-c0d8-402b-bc61-b79f92e9e6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from physicsnemo.registry import ModelRegistry\n",
    "\n",
    "model_registry = ModelRegistry()\n",
    "model_registry.__clear_registry__()\n",
    "model_registry.__restore_registry__()\n",
    "\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from physicsnemo.models.meta import ModelMetaData\n",
    "from physicsnemo.models.module import Module\n",
    "from the_well.benchmark.models import TFNO\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class WellMetaData(ModelMetaData):\n",
    "    name: str = \"WellTFNOModel\"\n",
    "\n",
    "\n",
    "well_nemo_model = Module.from_torch(TFNO, meta=WellMetaData())\n",
    "print(well_nemo_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54301d85-7faf-4ae0-8345-4991652edd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from physicsnemo.registry import ModelRegistry\n",
    "\n",
    "print(well_model.__dict__)\n",
    "well_nemo_model = ModelRegistry().factory(\"TFNOPhysicsNeMoModel\")(\n",
    "    dim_in=28,\n",
    "    dim_out=7,\n",
    "    n_spatial_dims=3,\n",
    "    spatial_resolution=[64, 64, 64],\n",
    "    hidden_channels=128,\n",
    "    modes1=16,\n",
    "    modes2=16,\n",
    "    modes3=16,\n",
    ")\n",
    "\n",
    "summary(well_nemo_model, depth=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4248391-ef8f-413e-a29b-5c307cb40e2b",
   "metadata": {},
   "source": [
    "The resulting `well_nemo_model` is simply the underlying PyTorch model from The Well, that can now be used with PhysicsNeMo and its utilities. Note that this is a newly instantiated model. To now train this model in PhysicsNeMo with the same dataset from The Well, we can update our `Trainer` to use this new model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a661e334-66e4-4f6c-ae6c-6ddf8fb0fd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHDTrainer(Trainer):\n",
    "    def setup_model(self):\n",
    "        \"\"\"Setup the TFNO model based in PhysicsNeMo. Input parameters are set and controlled by the yaml config.\"\"\"\n",
    "        from dataclasses import dataclass\n",
    "\n",
    "        from physicsnemo.models.meta import ModelMetaData\n",
    "        from physicsnemo.models.module import Module\n",
    "        from the_well.benchmark.models import TFNO\n",
    "\n",
    "        @dataclass\n",
    "        class WellMetaData(ModelMetaData):\n",
    "            name: str = \"WellTFNOModel\"\n",
    "\n",
    "        well_nemo_model = Module.from_torch(TFNO, meta=WellMetaData())\n",
    "        well_nemo_model = well_nemo_model(\n",
    "            dim_in=28,\n",
    "            dim_out=7,\n",
    "            n_spatial_dims=3,\n",
    "            spatial_resolution=[64, 64, 64],\n",
    "            hidden_channels=128,\n",
    "            modes1=16,\n",
    "            modes2=16,\n",
    "            modes3=16,\n",
    "        )\n",
    "\n",
    "        self.model = well_nemo_model.to(self.dist.device)\n",
    "        print(summary(self.model, depth=5))\n",
    "\n",
    "\n",
    "cfg.train_params.ckpt_path = \"./checkpoints/well_model_well_data\"\n",
    "well_model_trainer = MHDTrainer(cfg)\n",
    "well_model_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72959594-a700-4496-a66d-efbcf06b4a8a",
   "metadata": {},
   "source": [
    "## Fine-tuning Models from The Well in PhysicsNeMo\n",
    "Using the approach of converting models to PhysicsNeMo allows users to utilize all of the helpful features inside of PhysicsNeMo itself. One great feature of The Well is the abundance of pre-trained model that they provided, typically offering [many pretrained model architectures](https://huggingface.co/collections/polymathic-ai/the-well-benchmark-models-67e69bd7cd8e60229b5cd43e) for a single dataset. These pretrained models can be fine-tuned using PhysicsNeMo by leveraging a similar approach to the above example of converting a model into PhysicsNeMo format. In this section, the pretrained TFNO model will be loaded from The Well, converted to a PhysicsNeMo model, and then fine-tuned on a new dataset.\n",
    "\n",
    "The new dataset will come from a HuggingFace [community challenge focused around Stellerator design](https://huggingface.co/blog/cgeorgiaw/constellaration-fusion-challenge). In the field of magnetically confined fusion, the physics of magnetohydrodynamics influence the system in many ways, and modeling these equations help researchers and engineers understand reaction ability, equipment design, and plasma shapes, to name a few. In the remainder of this tutorial, we will set up the trainer, inspect the Stellerator desgin dataset, and fine-tune the model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9911444-643f-4756-ac71-78d6be097273",
   "metadata": {},
   "source": [
    "### Loading The Well Pre-Trained Checkpoint as PhysicsNeMo Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fa4ea3-c5f8-4741-bac4-9a847b150a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHDTrainer(Trainer):\n",
    "    def setup_model(self):\n",
    "        \"\"\"Setup the pretrained TFNO model from The Well as a PhysicsNeMo model.\n",
    "        Model input parameters are set from the well, and the original config is updated to reflect this.\"\"\"\n",
    "        import inspect\n",
    "\n",
    "        from physicsnemo.models.meta import ModelMetaData\n",
    "        from physicsnemo.models.module import Module\n",
    "        from the_well.benchmark.models import TFNO\n",
    "\n",
    "        well_model = TFNO.from_pretrained(\"polymathic-ai/TFNO-MHD_64\")\n",
    "        model_dict = well_model.__dict__\n",
    "\n",
    "        signature = inspect.signature(TFNO)\n",
    "        parameters = signature.parameters\n",
    "        filtered_params = {k: model_dict[k] for k in parameters if k in model_dict}\n",
    "\n",
    "        model = Module.from_torch(TFNO, meta=ModelMetaData(name=\"converted_tfno\"))\n",
    "        well_pretrained_model = model(**filtered_params)\n",
    "        well_pretrained_model.inner_model.load_state_dict(\n",
    "            well_model.state_dict(), strict=True\n",
    "        )\n",
    "        self.model = well_pretrained_model.to(self.dist.device)\n",
    "\n",
    "        print(summary(self.model, depth=4))\n",
    "        print(self.model)\n",
    "\n",
    "\n",
    "well_model_trainer = MHDTrainer(cfg)\n",
    "# well_model_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c05319-1bdd-400e-8b84-f96dd4faba04",
   "metadata": {},
   "source": [
    "### Exploring the HuggingFace Stellerator Design Dataset\n",
    "With a simple framework for using community datasets and models with PhysicsNeMo, some deeper problems can be explored. In this section, the pretrained TFNO model for magnetohydrodynamics will be augmented, and used as a foundation for transfer learning onto a new dataset with a related objective: optimizing the design of a stellarator. Stellarators are a type of magnetically confined fusion device for containing plasma of very high temperature and pressure. The design of these devices is complex due to thier geometry. A recent challenge on HuggingFace serves as the basis for the problem.\n",
    "\n",
    "\n",
    "From the challenge page:\n",
    "\n",
    "The ConStellaration dataset contains over 150,000 QI equilibria produced by VMEC++. As a reminder, QI stellarators are a subset of configurations that minimize the internal plasma currents that can lead to disruptive events in a tokamak. The provided equilibria correspond to different 3D plasma boundary surfaces and offer samples across a wide and physically meaningful range of parameters. The dataset includes:\n",
    "\n",
    "Input parameters: the 3D plasma boundary, together with the pressure and current profiles.\n",
    "Equilibrium outputs: full VMEC++ equilibrium solution plus additional metrics of interest for stellarator design (e.g., degree of QI symmetry, turbulent transport geometrical quantities).\n",
    "\n",
    "There are many ways in which to use the dataset, and namely there are three unique challenges present that are available to solve, starting with low complexity and moving up to multimodal optimization problems. While this example will not directly solve any of the stated challenges, it will serve as a starting point towards utilizing community models and datasets with PhysicsNeMo, to solve real world problems. \n",
    "\n",
    "The goal of the model in this case is to serve as a surrogate for approximating the equilibrium dynamics of the plasma, which then maps to quantities of interest. The dataset contains input geometry and a large list of output parameters that can be learned. Because the pretrained TFNO model already has learned knowledge of magnetohydrodynamics, it is used to translate the input design parameters into predicted properties, by way of mapping input geometry to predicted MHD field, and then onto the properties of interest. The model here will utilize the pretrained TFNO as a frozen intermediate representation, while the input and output projections will be learned during training. \n",
    "\n",
    "Specifically, the input geometry of the stellarator plasma is converted from Fourier coefficientes defining its boundaries into a 3D representation, and the output is the predicted max elongation of the plasma.\n",
    "\n",
    "The dataloader class is provided in `constellaration_dataloader.py1, and the model is described below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c778bfc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class TFNOSurrogate(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels=3,\n",
    "        output_dim=3,\n",
    "        hidden_dim=256,\n",
    "        target_input_channels=28,\n",
    "        target_output_channels=7,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.target_input_channels = target_input_channels\n",
    "        self.target_output_channels = target_output_channels\n",
    "\n",
    "        # Input projection: 3D conv to project from input_channels to target_input_channels\n",
    "        # This projects the 3D volume to match the pretrained TFNO's expected input\n",
    "        self.input_projection = nn.Sequential(\n",
    "            nn.Conv3d(input_channels, hidden_dim, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Conv3d(hidden_dim, hidden_dim // 2, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Conv3d(hidden_dim // 2, target_input_channels, kernel_size=1),\n",
    "        )\n",
    "\n",
    "        # Output projection: 3D conv to project from target_output_channels to output_dim\n",
    "        # This projects the TFNO output to our target output size\n",
    "        self.output_projection = nn.Sequential(\n",
    "            nn.Conv3d(target_output_channels, hidden_dim // 2, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Conv3d(hidden_dim // 2, hidden_dim, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Conv3d(\n",
    "                hidden_dim, 1, kernel_size=1\n",
    "            ),  # Single channel for global pooling\n",
    "        )\n",
    "\n",
    "        # Global pooling layer to convert 3D output to 1D\n",
    "        self.global_pool = nn.AdaptiveAvgPool3d(1)\n",
    "\n",
    "        # Final projection to output dimension\n",
    "        self.final_projection = nn.Sequential(\n",
    "            nn.Linear(1, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim // 2, output_dim),\n",
    "        )\n",
    "\n",
    "        self.surrogate = self.setup_model()  # pretrained TFNOPhysicsNeMoModel\n",
    "        self.freeze_surrogate_weights()\n",
    "\n",
    "    def setup_model(self):\n",
    "        \"\"\"Setup the pretrained TFNO model from The Well as a PhysicsNeMo model.\n",
    "        Model input parameters are set from the well, and the original config is updated to reflect this.\"\"\"\n",
    "        import inspect\n",
    "\n",
    "        from physicsnemo.models.meta import ModelMetaData\n",
    "        from physicsnemo.models.module import Module\n",
    "        from the_well.benchmark.models import TFNO\n",
    "\n",
    "        well_model = TFNO.from_pretrained(\"polymathic-ai/TFNO-MHD_64\")\n",
    "        model_dict = well_model.__dict__\n",
    "\n",
    "        signature = inspect.signature(TFNO)\n",
    "        parameters = signature.parameters\n",
    "        filtered_params = {k: model_dict[k] for k in parameters if k in model_dict}\n",
    "\n",
    "        model = Module.from_torch(TFNO, meta=ModelMetaData(name=\"converted_tfno\"))\n",
    "        well_pretrained_model = model(**filtered_params)\n",
    "        well_pretrained_model.inner_model.load_state_dict(\n",
    "            well_model.state_dict(), strict=True\n",
    "        )\n",
    "        return well_pretrained_model\n",
    "\n",
    "    def freeze_surrogate_weights(self):\n",
    "        \"\"\"Freeze all parameters in the pretrained TFNO surrogate model.\"\"\"\n",
    "        for param in self.surrogate.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Also freeze the surrogate model itself to prevent any updates\n",
    "        self.surrogate.eval()  # Set to evaluation mode\n",
    "\n",
    "        print(\n",
    "            f\"Frozen {sum(p.numel() for p in self.surrogate.parameters())} parameters in pretrained TFNO model\"\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input: [batch_size, channels, height, width, depth] from FullConstellarationDataset\n",
    "        # Expected: [batch_size, 3, 64, 64, 64] -> [batch_size, 28, 64, 64, 64]\n",
    "        # Input projection: 3D conv to match pretrained TFNO input channels\n",
    "        x_projected = self.input_projection(x)  # [batch_size, 28, 64, 64, 64]\n",
    "\n",
    "        # Pass through the pretrained TFNO model\n",
    "        tfno_output = self.surrogate(x_projected)  # [batch_size, 7, 64, 64, 64]\n",
    "\n",
    "        # Output projection: 3D conv to prepare for global pooling\n",
    "        tfno_projected = self.output_projection(\n",
    "            tfno_output\n",
    "        )  # [batch_size, 1, 64, 64, 64]\n",
    "\n",
    "        # Global average pooling: [batch_size, 1, 64, 64, 64] -> [batch_size, 1, 1, 1, 1]\n",
    "        pooled = self.global_pool(tfno_projected)  # [batch_size, 1, 1, 1, 1]\n",
    "\n",
    "        # Flatten: [batch_size, 1, 1, 1, 1] -> [batch_size, 1]\n",
    "        flattened = pooled.squeeze(-1).squeeze(-1).squeeze(-1)  # [batch_size, 1]\n",
    "\n",
    "        # Final projection to output dimension: [batch_size, 1] -> [batch_size, output_dim]\n",
    "        output = self.final_projection(flattened)  # [batch_size, 3]\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36431ba1",
   "metadata": {},
   "source": [
    "The model and dataloader can then be used with the previous training utilities and a few slight modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c7be03",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "class ConstellarationTrainer(Trainer):\n",
    "    def setup_model(self):\n",
    "        \"\"\"Setup the TFNO model based in PhysicsNeMo. Input parameters are set and controlled by the yaml config.\"\"\"\n",
    "        from constellaration_surrogate import TFNOSurrogate\n",
    "\n",
    "        self.model = TFNOSurrogate(input_channels=3, output_dim=1)\n",
    "        self.model.to(self.dist.device)\n",
    "\n",
    "    def _setup_data(self):\n",
    "        from constellaration_dataloader import ConstellarationDataLoader\n",
    "\n",
    "        self.datamodule = ConstellarationDataLoader(\n",
    "            challenge=\"challenge_1\",\n",
    "            dataset_type=\"full\",\n",
    "            grid_size=(64, 64, 64),\n",
    "            batch_size=2,\n",
    "        )\n",
    "        self.datamodule.prepare()\n",
    "\n",
    "    def _forward_pass(self, batch: List[torch.Tensor]) -> torch.Tensor:\n",
    "        # Add dimension for our one input field\n",
    "        inputs = batch[\"input_fields\"].unsqueeze(-1)\n",
    "        # Rearrange for model: (batch, time, x, y, z, fields) -> (batch, (time fields), x, y, z)\n",
    "        model_inputs = rearrange(inputs, \"B Ti Lx Ly Lz F -> B (Ti F) Lx Ly Lz\")\n",
    "\n",
    "        # Forward pass through model\n",
    "        model_outputs = self.model(model_inputs)\n",
    "        return model_outputs\n",
    "\n",
    "\n",
    "cfg.train_params.ckpt_path = \"./checkpoints/well_model_constellar_data\"\n",
    "constellaration_model_trainer = ConstellarationTrainer(cfg)\n",
    "constellaration_model_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33f1bea-cc2c-4cc5-af23-e605dbcadb75",
   "metadata": {},
   "source": [
    "## Expanding The Scope - More Models\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
