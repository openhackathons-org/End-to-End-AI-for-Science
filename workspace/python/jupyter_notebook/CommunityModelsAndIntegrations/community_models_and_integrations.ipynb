{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8005778a",
   "metadata": {},
   "source": [
    "# A Tutorial on Community Models and Integrations with PhysicsNeMo\n",
    "\n",
    "Scientific machine learning (SciML) is becoming a fundamental part of of research, development, and discovery workflows for scientists across many domains such as computational fluid dynamics, materials characterization and discover, climate and weather modeling, and computer aided engineering. With the increase use of AI and ML in these domains also comes an increase in the number of resources available for developing models, and a plethora of datasets to use as starting points for model training. While many efforts are segmented and tailor made for specific use cases, projects such as PhysicsNeMo, The Well, and ConStellaration are great examples of community efforts to bridge the gap between siloed SciML research and collaborative community projects. \n",
    "\n",
    "In this tutorial, PhysicsNeMo is used to expand the scope of community models and datasets from [The Well](https://github.com/PolymathicAI/the_well) and the [ConStellaration Challenge](https://huggingface.co/blog/cgeorgiaw/constellaration-fusion-challenge) by leveraging pretrained physics informed machine learning models, community accessible datasets, and inforporating MLOps best practices and engineering utilities through the robust SciML framework from PhysicsNeMo. Specifically, the tutorial covers:\n",
    "\n",
    "* Loading data from The Well\n",
    "* Training a PhysicsNeMo model using data from The Well\n",
    "* Running models from The Well in the PhysicsNeMo framework\n",
    "* Fine-tuning models from The Well using PhysicsNeMo\n",
    "* Exploring a community design challenge for fusion research and engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa944336",
   "metadata": {},
   "source": [
    "## Loading Models and Data from The Well\n",
    "\n",
    "The Well is large-scale collection of machine learning datasets containing numerical simulations of a wide variety of spatiotemporal physical systems. Additionally, a variety of state-of-the-art models are included, with a large selection of pre-trained models availabe for download on HuggingFace. Specifically, The Well includes 16 datasets covering diverse domains such as biological systems, fluid dynamics, acoustic scattering, and magnetohydrodynamics simulations. \n",
    "\n",
    "In this tutorial, the focus will largely be on magnetohydrodynamics. Magnetohydrodynamics (MHD), is the study of the dynamics of electrically conducting fluids such as plasmas. Its applications range from understanding the flow of plasmas in the Sun, to simulating the physics inside of magnetically confined fusion devices. The dynamics of these systems are represented by combining the Navier-Stokes equations with Maxwells equations - capturing both fluid flows and electromagnetic forces. \n",
    "\n",
    "\n",
    "While the specific domain focus of this tutorial is only on MDH and its applications, the workflows and integrations described are transferable to other domains, datasets and models. \n",
    "\n",
    "The tutorial will specifically make use of the [MHD_64 dataset](https://github.com/PolymathicAI/the_well/tree/master/datasets/MHD_64).  \n",
    "\n",
    "Datasets from The Well are available through HuggingFace, and can either be streamed via the `datasets` API, or saved locally. The command to download all splits is `the-well-download --base-path path/to/base --dataset MHD_64`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affec459",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "To quickly examine the dataset, a single sample can be loaded from the streaming API. The [datacard](https://huggingface.co/datasets/polymathic-ai/MHD_64) available on HuggingFace provides additional information about the simulations, equations modelled, and a reference paper for in depth explanation of the physics of the problem. A snipped for loading the dataset and details from the datacard are provided for reference.\n",
    "\n",
    "Note that the following snippet assumes the data has been downloaded locally, to utilize streaming: `use_streaming = True`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3381f0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from the_well.data import WellDataset\n",
    "from the_well.data.normalization import ZScoreNormalization\n",
    "\n",
    "use_streaming = False\n",
    "\n",
    "# Enable streaming the dataset from HuggingFace or loading from local directory\n",
    "# The following line may take a couple of minutes to instantiate the datamodule\n",
    "dataset = WellDataset(\n",
    "    well_base_path=\"hf://datasets/polymathic-ai/\"\n",
    "    if use_streaming\n",
    "    else \"./TheWellMHDData/datasets\",\n",
    "    well_dataset_name=\"MHD_64\",\n",
    "    well_split_name=\"train\",\n",
    "    use_normalization=True,\n",
    "    normalization_type=ZScoreNormalization,\n",
    "    n_steps_input=4,\n",
    "    n_steps_output=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc7787d",
   "metadata": {},
   "source": [
    "With the dataset on hand, its features, shape and size can be explored. The Well provides a great script for this already, and is left to the reader to go through if desired. A summary is provided below, also available on the [dataset card](https://github.com/PolymathicAI/the_well/blob/master/datasets/MHD_64/README.md) online:\n",
    "\n",
    "* **Dimension of discretized data:** 100 timesteps of 64 $\\times$ 64 $\\times$ 64 cubes.\n",
    "* **Fields available in the data:** Density (scalar field), velocity (vector field), magnetic field (vector field).\n",
    "* **Number of trajectories:** 10 Initial conditions x 10 combination of parameters = 100 trajectories.\n",
    "* **Estimated size of the ensemble of all simulations:** 71.6 GB.\n",
    "* **Grid type:** uniform grid, cartesian coordinates.\n",
    "* **Initial conditions:** uniform IC.\n",
    "* **Boundary conditions:** periodic boundary conditions.\n",
    "* **Data are stored separated by ($\\Delta t$):** 0.01 (arbitrary units).\n",
    "* **Total time range ($t\\_{min}$ to $t\\_{max}$):** $t\\_{min} = 0$, $t\\_{max} = 1$.\n",
    "* **Spatial domain size ($L_x$, $L_y$, $L_z$):** dimensionless so 64 pixels.\n",
    "* **Set of coefficients or non-dimensional parameters evaluated:** all combinations of $\\mathcal{M}_s=${0.5, 0.7, 1.5, 2.0 7.0} and $\\mathcal{M}_A =${0.7, 2.0}.\n",
    "* **Approximate time and hardware used to generate the data:** Downsampled from `MHD_256` after applying ideal low-pass filter.\n",
    "* **What phenomena of physical interest are catpured in the data:** MHD fluid flows in the compressible limit (sub and super sonic, sub and super Alfvenic).\n",
    "* **How to evaluate a new simulator operating in this space:** Check metrics such as Power spectrum, two-points correlation function.\n",
    "\n",
    "Please cite the associated paper if you use this data in your research:\n",
    "\n",
    "```\n",
    "@article{burkhart2020catalogue,\n",
    "  title={The catalogue for astrophysical turbulence simulations (cats)},\n",
    "  author={Burkhart, B and Appel, SM and Bialy, S and Cho, J and Christensen, AJ and Collins, D and Federrath, Christoph and Fielding, DB and Finkbeiner, D and Hill, AS and others},\n",
    "  journal={The Astrophysical Journal},\n",
    "  volume={905},\n",
    "  number={1},\n",
    "  pages={14},\n",
    "  year={2020},\n",
    "  publisher={IOP Publishing}\n",
    "}\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee44dbf1",
   "metadata": {},
   "source": [
    "A single sample can be extracted and inspected. Some notes from The Well are shown below. More info on examining their data can be found in [this example](https://github.com/PolymathicAI/the_well/blob/master/docs/tutorials/dataset.ipynb):\n",
    "\n",
    "The most important elements are `input_fields` and `output_fields`. They represent the time-varying physical fields of the dynamical system and are generally the input and target of our models. For a dynamical system that has 3 spatial dimensions $x$, $y$, and $z$, `input_fields` would have a shape $(T_{in}, L_x, L_y, L_z, F)$ and `output_fields` would have a shape $(T_{out}, L_x, L_y, L_z, F)$. The number of input and output timesteps $T_{in}$ and $T_{out}$ are specified at the instantiation of the dataset with the arguments `n_steps_input` and `n_steps_output`. $L_x$, $L_y$ and $L_z$ are the lengths of the spatial dimensions. $F$ represents the number of physical fields, where vector fields $v = (v_x, v_y, v_z)$ and tensor fields $t = (t_{xx}, t_{xy}, t_{xz}, t_{yy}, t_{yx}, t_{yz},  t_{zz}, t_{zx}, t_{zy})$ are flattened.\n",
    "\n",
    "Note that the MHD_64 dataset only contains scalar and vector fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ef513f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "sample = dataset[0]\n",
    "for k, v in sample.items():\n",
    "    print(f\"Key: {k.ljust(20)} Shape: {v.shape}\")\n",
    "\n",
    "print(f\"Field Names: {dataset.metadata.field_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0915ac36-ea41-4821-b38e-1e874cb8da12",
   "metadata": {},
   "source": [
    "## Using The Well Data to Train PhysicsNeMo Models\n",
    "\n",
    "With a dataset on hand, a model can be trained to approximate the system dynamics, resulting in a surrogate model for magnetohydrodyanmics simulations. To align with models from The Well, the PhysicsNeMo Tensor-Factorized Fourier Neural Operator implementation is adapted to closely match the implementation from The Well. The model utilizes Tucker factorization, and has around 300M parameters. \n",
    "\n",
    "For the remainder of this tutorial, a boiler-plate `Trainer` class is implemented in `training_utils.py` that contains the core components needed for loading models, running training loops, saving checkpoints, evaluating models, etc. The method `setup_model` needs to be implemented in order to attach a model to the Trainer. For completeness, the TFNO architecture from `PhysicsNeMo` is included in the `tfno` folder, which is initialized with parameters that closely match with the defaults from The Well. The config file used to define the parameters for our model, data, and trainer are in the `config` folder.\n",
    "\n",
    "The PhysicsNeMo framework utilizes the Hydra configuration framework, enabling streamlined tracking of all parameteres associated with running experiments, and is a common best-practice in machine learning workflows. Additionally, the `training_utils.py` makes use of other best-practices such as metric logging, checkpointing, and distributed workflow utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfd6ad7-efaf-4113-82f8-0f79d4e46b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "from hydra import compose, initialize\n",
    "from training_utils import Trainer\n",
    "\n",
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "\n",
    "initialize(version_base=None, config_path=\"./config\")\n",
    "cfg = compose(config_name=\"mhd_config.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41775e6",
   "metadata": {},
   "source": [
    "\n",
    "Note that the `training_utils.py` script can be used directly by calling it directly, or ideally through `torchrun`, however the `setup_model` method will need to be implemented.\n",
    "```python \n",
    "python training_utils.py\n",
    "```\n",
    "or \n",
    "```python\n",
    "torchrun --standalone --nnodes=1 --nproc_per_node=1 training_utils.py\n",
    "```\n",
    "\n",
    "In the following code, the TFNO model from PhysicsNeMo is set up in a child class of the `Trainer`, which instantiates the model through various model parameters set in the Hydra config file. A model summary is printed for convienence and reference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09dee18-82aa-474f-8cb8-6d0de3cc5203",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHDTrainer(Trainer):\n",
    "    def setup_model(self):\n",
    "        \"\"\"Setup the TFNO model based in PhysicsNeMo. Input parameters are set and controlled by the yaml config.\"\"\"\n",
    "        from tfno.tfno import TFNO\n",
    "\n",
    "        self.model = TFNO(\n",
    "            in_channels=self.model_params.in_dim,\n",
    "            coord_features=self.model_params.coord_features,\n",
    "            out_channels=self.model_params.out_dim,\n",
    "            decoder_layers=self.model_params.decoder_layers,\n",
    "            decoder_layer_size=self.model_params.fc_dim,\n",
    "            dimension=self.model_params.dimension,\n",
    "            latent_channels=self.model_params.layers,\n",
    "            num_fno_layers=self.model_params.num_fno_layers,\n",
    "            num_fno_modes=self.model_params.modes,\n",
    "            padding=[\n",
    "                self.model_params.pad_z,\n",
    "                self.model_params.pad_y,\n",
    "                self.model_params.pad_x,\n",
    "            ],\n",
    "            rank=self.model_params.rank,\n",
    "            factorization=self.model_params.factorization,\n",
    "            fixed_rank_modes=self.model_params.fixed_rank_modes,\n",
    "            decomposition_kwargs=self.model_params.decomposition_kwargs,\n",
    "        ).to(self.dist.device)\n",
    "\n",
    "        print(summary(self.model, depth=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab91f1e",
   "metadata": {},
   "source": [
    "Finally, the model can be trained. Note that one of the config parameters is overwritten here for convienence, which could have optionally be set directly in the `.yaml` file itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d158042-3c58-4079-8196-206ba038f9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.train_params.ckpt_path = \"./checkpoints/pnm_model_well_data\"\n",
    "mhd_trainer = MHDTrainer(cfg)\n",
    "mhd_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876d75b9",
   "metadata": {},
   "source": [
    "A the end of model training, checkpoints from the specified `ckpt_freq` are saved to the path above, which can be loaded with the same `MDHTrainer` to use for fine-tuint, inference or evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc37d72-df24-4944-8d3a-37dcc0f65758",
   "metadata": {},
   "source": [
    "## Running Models From The Well in PhysicsNemo\n",
    "\n",
    "PhysicsNeMo offers great utility in running community models natively in PhysicsNeMo by way of a simple conversion. The documentation for this can be found [here](https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/api/physicsnemo.models.html#converting-pytorch-models-to-physicsnemo-models), and is outlined below. Before working through the conversion process, the model interface to The Well is explored to understand how these models are used in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3a9ec3",
   "metadata": {},
   "source": [
    "Models from The Well can be loaded through their benchmark API. The vanilla PyTorch model is used with the `Module.from_torch` method to pull in the model to the PhysicsNeMo framework. The `ModelMetaData` class allows for various optimization strategies to be inclued in this conversion, including just-in-time compilation, automatic mixed precision training, and cuda-graphs.\n",
    "\n",
    "```python\n",
    "@dataclass\n",
    "class GenericModelMetadata(ModelMetaData):\n",
    "    name: str = \"GenericModel\"\n",
    "    # Optimization\n",
    "    jit: bool = True\n",
    "    cuda_graphs: bool = True\n",
    "    amp_cpu: bool = True\n",
    "    amp_gpu: bool = True\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af4d77d-c0d8-402b-bc61-b79f92e9e6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "from physicsnemo.models.meta import ModelMetaData\n",
    "from physicsnemo.models.module import Module\n",
    "from the_well.benchmark.models import TFNO\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class WellMetaData(ModelMetaData):\n",
    "    name: str = \"WellTFNOModel\"\n",
    "\n",
    "\n",
    "well_nemo_model = Module.from_torch(TFNO, meta=WellMetaData())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f1c1b9",
   "metadata": {},
   "source": [
    "At this stage, the `well_nemo_model` still needs to be initialized with model parameters. We can simply use:\n",
    "```python\n",
    "instantiated_model = well_nemo_model(**parameters)\n",
    "```\n",
    "\n",
    "Alternatively, the PhysicsNeMo model registry can be used to load in the newly created model.\n",
    "\n",
    "For reference, the model summary is displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54301d85-7faf-4ae0-8345-4991652edd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from physicsnemo.registry import ModelRegistry\n",
    "\n",
    "print(well_model.__dict__)\n",
    "well_nemo_model = ModelRegistry().factory(\"TFNOPhysicsNeMoModel\")(\n",
    "    dim_in=28,\n",
    "    dim_out=7,\n",
    "    n_spatial_dims=3,\n",
    "    spatial_resolution=[64, 64, 64],\n",
    "    hidden_channels=128,\n",
    "    modes1=16,\n",
    "    modes2=16,\n",
    "    modes3=16,\n",
    ")\n",
    "\n",
    "summary(well_nemo_model, depth=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8351d103",
   "metadata": {},
   "source": [
    "By examining the model summary from above and the order of operations in the [TFNO forward pass](https://github.com/neuraloperator/neuraloperator/blob/7bb578df787d6ca548b623b83f0601c98dc931fb/neuralop/models/fno.py#L337), the data is processed by:\n",
    "\n",
    "1. Applying optional positional encoding\n",
    "2. Sending inputs through a lifting layer to a high-dimensional latent space\n",
    "3. Applying optional domain padding to high-dimensional intermediate function representation\n",
    "4. Applying `n_layers` Fourier/TFNO layers in sequence (SpectralConvolution + skip connections, nonlinearity) \n",
    "5. If domain padding was applied, domain padding is removed\n",
    "6. Projection of intermediate function representation to the output channels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77219d3f",
   "metadata": {},
   "source": [
    "The Well benchmark aims to showcase the effectiveness of applyging SoTA models to the forward problem: predicting the enxt step of a simulation from a histor of 4 previous timesteps. Additionally, autoregressive rollouts are investigated. \n",
    "\n",
    "Concretely, the MHD TFNO model is used to predict the $T_{out} = 1$ next states given the $T_{in} = 4$ previous states. The input steps are concatenated along their channels, such that the model expects $T_{in} \\times F$ channels as input and $T_{out} \\times F$ channels as output. This introduces an assumption of our models that four timesteps will be avaialbe to use as input and can limit the application of these models to real world use cases, however this serves as a convienent starting point towards building surrogate modesl for this dyanamical system. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4248391-ef8f-413e-a29b-5c307cb40e2b",
   "metadata": {},
   "source": [
    "To now train this model in PhysicsNeMo with the same dataset from The Well, the `Trainer` class can be updated to perform the same loading and conversion that is required to pull the model into PhysicsNeMo framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a661e334-66e4-4f6c-ae6c-6ddf8fb0fd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHDTrainer(Trainer):\n",
    "    def setup_model(self):\n",
    "        \"\"\"Setup the TFNO model based in PhysicsNeMo. Input parameters are set and controlled by the yaml config.\"\"\"\n",
    "        from dataclasses import dataclass\n",
    "\n",
    "        from physicsnemo.models.meta import ModelMetaData\n",
    "        from physicsnemo.models.module import Module\n",
    "        from the_well.benchmark.models import TFNO\n",
    "\n",
    "        @dataclass\n",
    "        class WellMetaData(ModelMetaData):\n",
    "            name: str = \"WellTFNOModel\"\n",
    "\n",
    "        well_nemo_model = Module.from_torch(TFNO, meta=WellMetaData())\n",
    "        well_nemo_model = well_nemo_model(\n",
    "            dim_in=28,\n",
    "            dim_out=7,\n",
    "            n_spatial_dims=3,\n",
    "            spatial_resolution=[64, 64, 64],\n",
    "            hidden_channels=128,\n",
    "            modes1=16,\n",
    "            modes2=16,\n",
    "            modes3=16,\n",
    "        )\n",
    "\n",
    "        self.model = well_nemo_model.to(self.dist.device)\n",
    "        print(summary(self.model, depth=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc308aaf",
   "metadata": {},
   "source": [
    "Train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e0a475",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.train_params.ckpt_path = \"./checkpoints/well_model_well_data\"\n",
    "well_model_trainer = MHDTrainer(cfg)\n",
    "well_model_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eda699",
   "metadata": {},
   "source": [
    "Similar to the previous section, checkpoints are available for the trained model in the specififed directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72959594-a700-4496-a66d-efbcf06b4a8a",
   "metadata": {},
   "source": [
    "## Fine-tuning Models from The Well in PhysicsNeMo\n",
    "One great feature of The Well is the abundance of pre-trained model that they provided, typically offering [many pretrained model architectures](https://huggingface.co/collections/polymathic-ai/the-well-benchmark-models-67e69bd7cd8e60229b5cd43e) for a single dataset. These pretrained models can be fine-tuned using PhysicsNeMo by leveraging a similar approach to the above example of converting a model into PhysicsNeMo format. Using the approach of converting models to PhysicsNeMo allows users to utilize all of the helpful features inside of PhysicsNeMo itself, and is a great use case of integrating community models, data and frameworks into a unified interface.\n",
    "\n",
    "In this section, the pretrained TFNO model will be loaded from The Well, converted to a PhysicsNeMo model, and then used in the context of transfer learning to adapt the model to a new task with yet another community dataset.\n",
    "\n",
    "The new dataset will come from a HuggingFace [community challenge focused around Stellerator design](https://huggingface.co/blog/cgeorgiaw/constellaration-fusion-challenge). In the field of magnetically confined fusion, the physics of magnetohydrodynamics influence the system in many ways, and modeling these equations help researchers and engineers understand reaction ability, equipment design, and plasma shapes, to name a few. In the remainder of this tutorial, we will set up the trainer, inspect the Stellerator desgin dataset, and fine-tune the model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9911444-643f-4756-ac71-78d6be097273",
   "metadata": {},
   "source": [
    "### Loading The Well Pre-Trained Checkpoint as PhysicsNeMo Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4de9c4",
   "metadata": {},
   "source": [
    "Models from The Well benchmark are available thorugh HuggingFace, and can be trivially loaded with just a few lines of code:\n",
    "\n",
    "```python\n",
    "from the_well.benchmark.models import TFNO\n",
    "\n",
    "well_model = TFNO.from_pretrained(\"polymathic-ai/TFNO-MHD_64\")\n",
    "```\n",
    "\n",
    "In order to use this model with PhysicsNeMo, the base TFNO pytorch model needs to first be converted to PhysicsNeMo format in the same way as the previous section. After this, the pre-trained model parameters can be directly transfered to the PhysicsNeMo model. To ensure the model is instantiated correctly the following steps can be used:\n",
    "* Collect all of the model hyperparameters from the pre-trained model instance\n",
    "* Inspect the TFNO class for its required input arguments\n",
    "* Instantiate the PhysicsNeMo model with the required TFNO arguments and their values from the pre-trained model.\n",
    "\n",
    "The explicit steps for this procedue are outlined in the `setup_model` method of `MHDTrainer` below. \n",
    "\n",
    "Note that this procedure is general and can be applied other models from within The Well, as well as additional community models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fa4ea3-c5f8-4741-bac4-9a847b150a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHDTrainer(Trainer):\n",
    "    def setup_model(self):\n",
    "        \"\"\"Setup the pretrained TFNO model from The Well as a PhysicsNeMo model.\n",
    "        Model input parameters are set from the well, and the original config is updated to reflect this.\"\"\"\n",
    "        import inspect\n",
    "\n",
    "        from physicsnemo.models.meta import ModelMetaData\n",
    "        from physicsnemo.models.module import Module\n",
    "        from the_well.benchmark.models import TFNO\n",
    "\n",
    "        well_model = TFNO.from_pretrained(\"polymathic-ai/TFNO-MHD_64\")\n",
    "        model_dict = well_model.__dict__\n",
    "\n",
    "        signature = inspect.signature(TFNO)\n",
    "        parameters = signature.parameters\n",
    "        filtered_params = {k: model_dict[k] for k in parameters if k in model_dict}\n",
    "\n",
    "        model = Module.from_torch(TFNO, meta=ModelMetaData(name=\"converted_tfno\"))\n",
    "        well_pretrained_model = model(**filtered_params)\n",
    "        well_pretrained_model.inner_model.load_state_dict(\n",
    "            well_model.state_dict(), strict=True\n",
    "        )\n",
    "        self.model = well_pretrained_model.to(self.dist.device)\n",
    "\n",
    "        print(summary(self.model, depth=4))\n",
    "        print(self.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a25d60",
   "metadata": {},
   "source": [
    "With a community pre-trained model now available trough the PhysicsNeMo framework, it can now be applied to downstream tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c05319-1bdd-400e-8b84-f96dd4faba04",
   "metadata": {},
   "source": [
    "### Exploring the HuggingFace Stellerator Design Dataset\n",
    "With resources for a simple framework for using community datasets and models with PhysicsNeMo, some deeper problems can be explored. In this section, the pretrained TFNO model for magnetohydrodynamics will be augmented, and used as a foundation for transfer learning onto a new dataset with a related objective: optimizing the design of a stellarator. Stellarators are a type of magnetically confined fusion device for containing plasma of very high temperature and pressure. The design of these devices is complex due to thier geometry. A recent challenge on HuggingFace serves as the basis for this transfer learning problem.\n",
    "\n",
    "From the challenge page:\n",
    "\n",
    "*\"The ConStellaration dataset contains over 150,000 QI equilibria produced by VMEC++. As a reminder, QI stellarators are a subset of configurations that minimize the internal plasma currents that can lead to disruptive events in a tokamak. The provided equilibria correspond to different 3D plasma boundary surfaces and offer samples across a wide and physically meaningful range of parameters. The dataset includes:*\n",
    "\n",
    "*Input parameters: the 3D plasma boundary, together with the pressure and current profiles.*\n",
    "*Equilibrium outputs: full VMEC++ equilibrium solution plus additional metrics of interest for stellarator design (e.g., degree of QI symmetry, turbulent transport geometrical quantities).\"*\n",
    "\n",
    "\n",
    "There are many ways in which to use the dataset, and namely there are three unique challenges present that are available to solve, starting with low complexity and moving up to multimodal optimization problems. The three challenge problems involve optimizing the stellarator geomerty to minimize or maximize certain related parameters. such as minimizing elongation under fixed constraints, optimizing plasma shapes for confinement, and balancing compactness and simplicity.\n",
    "\n",
    "\n",
    "These challenges serve a great North Start for combing these multiple frameworks, and while this example will not directly solve any of the stated challenges, it will serve as a starting point towards utilizing community models and datasets with PhysicsNeMo to solve real world problems.\n",
    "\n",
    "The goal of the model in this case is to serve as a surrogate for approximating the equilibrium dynamics of the plasma, which then maps to quantities of interest. The dataset contains input geometry and a large list of output parameters that can be learned. Because the pretrained TFNO model already has learned knowledge of magnetohydrodynamics, it is used to translate the input design parameters into predicted properties, by way of mapping input geometry to predicted MHD equilibrium field, and then onto the properties of interest. The model here will utilize the pretrained TFNO as a frozen intermediate surrogate model, while the input and output projections will be learned during training. \n",
    "\n",
    "Specifically, the input geometry of the stellarator plasma is converted from Fourier coefficientes defining its boundaries into a 3D representation, and the output is the predicted max elongation of the plasma."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173ec6e6",
   "metadata": {},
   "source": [
    "The dataloader class is provided in `constellaration_dataloader.py` provides a base impementation of dataloaders and datasets that can be used to start working on machine learning models for predicting properties of stellarators. The `FullConstellarationDataLoader` transforms the Fourier coefficients in to a 3D represenatation of the stellarator boundary, and the `ConstellarationDataLoader` simply returns a concatenated list of the coefficients. In this example, the mapping of Fourier coefficients to 3D space is utilized to ensure closer alignment with the intermediate surrogate model. An example from the dataset is loaded and visualized below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce38ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from constellaration_dataloader import ConstellarationDataLoader\n",
    "\n",
    "# Initialize the dataloader for challenge_1 with full dataset type\n",
    "dataloader = ConstellarationDataLoader(\n",
    "    challenge=\"challenge_1\",\n",
    "    dataset_type=\"full\",\n",
    "    grid_size=(64, 64, 64),  # Smaller grid for faster processing\n",
    "    batch_size=1,\n",
    ")\n",
    "\n",
    "# Prepare the dataset\n",
    "dataloader.prepare()\n",
    "\n",
    "# Get a single sample from the training dataset\n",
    "sample = dataloader.train_dataset[0]\n",
    "\n",
    "# Extract the 3D volume data and metric value\n",
    "input_fields = sample[\"input_fields\"]  # Shape: [3, Nt, Np, Nrho]\n",
    "output_fields = sample[\"output_fields\"]  # Shape: [1] for challenge_1\n",
    "\n",
    "# Get the metric value (max_elongation for challenge_1)\n",
    "max_elongation = output_fields.item()\n",
    "\n",
    "# Extract the surface points (rho=1, which is the last index)\n",
    "# The input_fields contains [X, Y, Z] coordinates\n",
    "X_surface = input_fields[0, :, :, -1].numpy()  # X coordinates at surface\n",
    "Y_surface = input_fields[1, :, :, -1].numpy()  # Y coordinates at surface\n",
    "Z_surface = input_fields[2, :, :, -1].numpy()  # Z coordinates at surface\n",
    "\n",
    "# Create the 3D plot\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "# Plot the surface\n",
    "surface = ax.plot_surface(\n",
    "    X_surface,\n",
    "    Y_surface,\n",
    "    Z_surface,\n",
    "    cmap=\"viridis\",\n",
    "    alpha=0.8,\n",
    "    linewidth=0,\n",
    "    antialiased=True,\n",
    ")\n",
    "\n",
    "# Add colorbar\n",
    "fig.colorbar(surface, ax=ax, shrink=0.5, aspect=5)\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel(\"X\")\n",
    "ax.set_ylabel(\"Y\")\n",
    "ax.set_zlabel(\"Z\")\n",
    "ax.set_title(f\"3D Stellarator Surface - Max Elongation: {max_elongation:.4f}\")\n",
    "\n",
    "ax.view_init(elev=20, azim=45)\n",
    "ax.set_box_aspect([1, 1, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.savefig(\"3D_Stellarator_Surface.png\")\n",
    "\n",
    "# Print some information about the sample\n",
    "print(f\"Sample metric (max_elongation): {max_elongation:.4f}\")\n",
    "print(f\"Surface shape: {X_surface.shape}\")\n",
    "print(f\"X range: [{X_surface.min():.3f}, {X_surface.max():.3f}]\")\n",
    "print(f\"Y range: [{Y_surface.min():.3f}, {Y_surface.max():.3f}]\")\n",
    "print(f\"Z range: [{Z_surface.min():.3f}, {Z_surface.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadd27fa",
   "metadata": {},
   "source": [
    "The visualization shows the surface of the stellarator plasma boundary that has an associated `max_elongation` property associated with it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cef8ed",
   "metadata": {},
   "source": [
    "The model used to predict `max_elongation` from the input plasm boundary surface is provided below. The pre-trained model from The Well is utilized as an intermediate surrogate model for approximaitng the equilibrium dynamics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c778bfc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class TFNOSurrogate(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels=3,\n",
    "        output_dim=3,\n",
    "        hidden_dim=256,\n",
    "        target_input_channels=28,\n",
    "        target_output_channels=7,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.target_input_channels = target_input_channels\n",
    "        self.target_output_channels = target_output_channels\n",
    "\n",
    "        # Input projection: 3D conv to project from input_channels to target_input_channels\n",
    "        # This projects the 3D volume to match the pretrained TFNO's expected input\n",
    "        self.input_projection = nn.Sequential(\n",
    "            nn.Conv3d(input_channels, hidden_dim, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Conv3d(hidden_dim, hidden_dim // 2, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Conv3d(hidden_dim // 2, target_input_channels, kernel_size=1),\n",
    "        )\n",
    "\n",
    "        # Output projection: 3D conv to project from target_output_channels to output_dim\n",
    "        # This projects the TFNO output to our target output size\n",
    "        self.output_projection = nn.Sequential(\n",
    "            nn.Conv3d(target_output_channels, hidden_dim // 2, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Conv3d(hidden_dim // 2, hidden_dim, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Conv3d(\n",
    "                hidden_dim, 1, kernel_size=1\n",
    "            ),  # Single channel for global pooling\n",
    "        )\n",
    "\n",
    "        # Global pooling layer to convert 3D output to 1D\n",
    "        self.global_pool = nn.AdaptiveAvgPool3d(1)\n",
    "\n",
    "        # Final projection to output dimension\n",
    "        self.final_projection = nn.Sequential(\n",
    "            nn.Linear(1, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim // 2, output_dim),\n",
    "        )\n",
    "\n",
    "        self.surrogate = self.setup_model()  # pretrained TFNOPhysicsNeMoModel\n",
    "        self.freeze_surrogate_weights()\n",
    "\n",
    "    def setup_model(self):\n",
    "        \"\"\"Setup the pretrained TFNO model from The Well as a PhysicsNeMo model.\n",
    "        Model input parameters are set from the well, and the original config is updated to reflect this.\"\"\"\n",
    "        import inspect\n",
    "\n",
    "        from physicsnemo.models.meta import ModelMetaData\n",
    "        from physicsnemo.models.module import Module\n",
    "        from the_well.benchmark.models import TFNO\n",
    "\n",
    "        well_model = TFNO.from_pretrained(\"polymathic-ai/TFNO-MHD_64\")\n",
    "        model_dict = well_model.__dict__\n",
    "\n",
    "        signature = inspect.signature(TFNO)\n",
    "        parameters = signature.parameters\n",
    "        filtered_params = {k: model_dict[k] for k in parameters if k in model_dict}\n",
    "\n",
    "        model = Module.from_torch(TFNO, meta=ModelMetaData(name=\"converted_tfno\"))\n",
    "        well_pretrained_model = model(**filtered_params)\n",
    "        well_pretrained_model.inner_model.load_state_dict(\n",
    "            well_model.state_dict(), strict=True\n",
    "        )\n",
    "        return well_pretrained_model\n",
    "\n",
    "    def freeze_surrogate_weights(self):\n",
    "        \"\"\"Freeze all parameters in the pretrained TFNO surrogate model.\"\"\"\n",
    "        for param in self.surrogate.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Also freeze the surrogate model itself to prevent any updates\n",
    "        self.surrogate.eval()  # Set to evaluation mode\n",
    "\n",
    "        print(\n",
    "            f\"Frozen {sum(p.numel() for p in self.surrogate.parameters())} parameters in pretrained TFNO model\"\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input: [batch_size, channels, height, width, depth] from FullConstellarationDataset\n",
    "        # Expected: [batch_size, 3, 64, 64, 64] -> [batch_size, 28, 64, 64, 64]\n",
    "        # Input projection: 3D conv to match pretrained TFNO input channels\n",
    "        x_projected = self.input_projection(x)  # [batch_size, 28, 64, 64, 64]\n",
    "\n",
    "        # Pass through the pretrained TFNO model\n",
    "        tfno_output = self.surrogate(x_projected)  # [batch_size, 7, 64, 64, 64]\n",
    "\n",
    "        # Output projection: 3D conv to prepare for global pooling\n",
    "        tfno_projected = self.output_projection(\n",
    "            tfno_output\n",
    "        )  # [batch_size, 1, 64, 64, 64]\n",
    "\n",
    "        # Global average pooling: [batch_size, 1, 64, 64, 64] -> [batch_size, 1, 1, 1, 1]\n",
    "        pooled = self.global_pool(tfno_projected)  # [batch_size, 1, 1, 1, 1]\n",
    "\n",
    "        # Flatten: [batch_size, 1, 1, 1, 1] -> [batch_size, 1]\n",
    "        flattened = pooled.squeeze(-1).squeeze(-1).squeeze(-1)  # [batch_size, 1]\n",
    "\n",
    "        # Final projection to output dimension: [batch_size, 1] -> [batch_size, output_dim]\n",
    "        output = self.final_projection(flattened)  # [batch_size, 3]\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36431ba1",
   "metadata": {},
   "source": [
    "The model and dataloader can then be used with the previous training utilities and a few slight modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c7be03",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "class ConstellarationTrainer(Trainer):\n",
    "    def setup_model(self):\n",
    "        \"\"\"Setup the TFNO model based in PhysicsNeMo. Input parameters are set and controlled by the yaml config.\"\"\"\n",
    "        from constellaration_surrogate import TFNOSurrogate\n",
    "\n",
    "        self.model = TFNOSurrogate(input_channels=3, output_dim=1)\n",
    "        self.model.to(self.dist.device)\n",
    "\n",
    "    def _setup_data(self):\n",
    "        from constellaration_dataloader import ConstellarationDataLoader\n",
    "\n",
    "        self.datamodule = ConstellarationDataLoader(\n",
    "            challenge=\"challenge_1\",\n",
    "            dataset_type=\"full\",\n",
    "            grid_size=(64, 64, 64),\n",
    "            batch_size=2,\n",
    "        )\n",
    "        self.datamodule.prepare()\n",
    "\n",
    "    def _forward_pass(self, batch: List[torch.Tensor]) -> torch.Tensor:\n",
    "        # Add dimension for our one input field\n",
    "        inputs = batch[\"input_fields\"].unsqueeze(-1)\n",
    "        # Rearrange for model: (batch, time, x, y, z, fields) -> (batch, (time fields), x, y, z)\n",
    "        model_inputs = rearrange(inputs, \"B Ti Lx Ly Lz F -> B (Ti F) Lx Ly Lz\")\n",
    "\n",
    "        # Forward pass through model\n",
    "        model_outputs = self.model(model_inputs)\n",
    "        return model_outputs\n",
    "\n",
    "\n",
    "cfg.train_params.ckpt_path = \"./checkpoints/well_model_constellar_data\"\n",
    "constellaration_model_trainer = ConstellarationTrainer(cfg)\n",
    "constellaration_model_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33f1bea-cc2c-4cc5-af23-e605dbcadb75",
   "metadata": {},
   "source": [
    "## Expanding The Scope - More Models\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
